{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make that we are running the correct version of TensorFlow first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 6) # Python ≥3.6 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train basic model on the generated emnist-lines dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "# To be able to write code as if this notebook was one level up in files tree structure.\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model as KerasModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recognizer.datasets import Dataset\n",
    "from recognizer.datasets import EmnistDataset\n",
    "from recognizer.networks import lenet\n",
    "from recognizer.networks import simple\n",
    "from recognizer.networks import NetworkInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = Path(\"../recognizer/weights/lines_all_conv_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load characters mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(Dataset.raw_data_path()/\"emnist\"/\"emnist_essentials.json\") as json_file:  \n",
    "    mapping = json.load(json_file)[\"mapping\"]\n",
    "    mapping = {m[0]: m[1] for m in mapping}\n",
    "    mapping[62] = ' '\n",
    "    mapping[63] = '_'\n",
    "    \n",
    "mapping_reversed = {v: k for k, v in mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_image_height = 28\n",
    "char_image_width = 28\n",
    "max_length = 34\n",
    "num_classes = 64\n",
    "image_height = char_image_height\n",
    "image_width = char_image_width * max_length\n",
    "\n",
    "input_shape = (image_height, image_width)\n",
    "output_shape = (max_length, num_classes)\n",
    "\n",
    "window_width: float = 16\n",
    "window_stride: float = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # model.add(layers.Reshape((image_height, image_width, 1), input_shape=input_shape))\n",
    "\n",
    "    # # Normalize the image\n",
    "    # model.add(layers.Lambda(lambda x: norm(x, network_input.mean, network_input.std), input_shape=input_shape, output_shape=input_shape))\n",
    "\n",
    "    model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu' , input_shape=(image_height, image_width, 1)))\n",
    "    model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    # (image_height // 2 - 2, image_width // 2 - 2, 64)\n",
    "\n",
    "    # -2 because we use the default padding which is 'valid' not 'same'.\n",
    "    # /2 because we did maxPooling with pool size 2, this halved the output size of the previous conv layer.\n",
    "    new_height = image_height // 2 - 2\n",
    "    new_width = image_width // 2 - 2\n",
    "    # Let us also scale the desired window width and stride accordingly.\n",
    "    new_window_width = window_width // 2 - 2\n",
    "    new_window_stride = window_stride // 2\n",
    "    model.add(layers.Conv2D(128, kernel_size=(new_height, new_window_width), strides=(1, new_window_stride), activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    # (1, num_windows, 128)\n",
    "\n",
    "    num_windows = int((new_width - new_window_width) / new_window_stride) + 1\n",
    "    model.add(layers.Reshape((num_windows, 128, 1)))\n",
    "    # (num_windows, 128, 1)\n",
    "\n",
    "    width = int(num_windows / max_length)\n",
    "    model.add(layers.Conv2D(num_classes, kernel_size=(width, 128), strides=(width, 1), activation='softmax'))\n",
    "    # (image_width / width, 1, num_classes)\n",
    "\n",
    "    model.add(layers.Lambda(lambda x: tf.squeeze(x, 2)))\n",
    "    # (max_length, num_classes)\n",
    "\n",
    "    # Since we floor'd the calculation of width, we might have too many items in the sequence. Take only output_length.\n",
    "    model.add(layers.Lambda(lambda x: x[:, :max_length, :]))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0825 10:41:45.061343 4461979072 deprecation.py:323] From /Users/mostafagazar/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "if model_save_path.exists():\n",
    "    print(\"Load saved model\")\n",
    "    \n",
    "    model = keras.models.load_model(model_save_path)\n",
    "else:\n",
    "    print(\"Build new model\")\n",
    "    \n",
    "    model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 950, 32)       320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 948, 64)       18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 474, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 474, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 118, 128)       589952    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 118, 128)       0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 118, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 39, 1, 64)         24640     \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 39, 64)            0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 34, 64)            0         \n",
      "=================================================================\n",
      "Total params: 633,408\n",
      "Trainable params: 633,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Dataset.processed_data_path()/\"emnist_lines\"/\"data.csv\")\n",
    "\n",
    "output_length = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.png</td>\n",
       "      <td>Whats wrong at state _____________</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.png</td>\n",
       "      <td>The transducer itself moves the __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.png</td>\n",
       "      <td>There is an ancient and venerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.png</td>\n",
       "      <td>They destroyed a trading house ___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.png</td>\n",
       "      <td>Alec waited a moment on guard  ___</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image                            sentence\n",
       "0  0.png  Whats wrong at state _____________\n",
       "1  1.png  The transducer itself moves the __\n",
       "2  2.png  There is an ancient and venerable \n",
       "3  3.png  They destroyed a trading house ___\n",
       "4  4.png  Alec waited a moment on guard  ___"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=952x28 at 0x6311D1E80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/\"0.png\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 952)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 34, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_numpy = np.array(image).astype(np.float32)\n",
    "test_batch = image_numpy.reshape(1, image_height, image_width, 1)\n",
    "\n",
    "preds = model(test_batch)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=952x28 at 0x6311D1E80>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " he s        _  _st   e __________"
     ]
    }
   ],
   "source": [
    "for char_pred in preds[0]:\n",
    "    print(mapping[np.argmax(char_pred)], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the data and reset their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8582.png</td>\n",
       "      <td>Now he was just in the late poems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2236.png</td>\n",
       "      <td>That boat in the river maybe  ____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4353.png</td>\n",
       "      <td>They condemned the movie script __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1785.png</td>\n",
       "      <td>Some liars logic a wisp of _______</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2703.png</td>\n",
       "      <td>During the first pass of Phase 3 _</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image                            sentence\n",
       "0  8582.png  Now he was just in the late poems \n",
       "1  2236.png  That boat in the river maybe  ____\n",
       "2  4353.png  They condemned the movie script __\n",
       "3  1785.png  Some liars logic a wisp of _______\n",
       "4  2703.png  During the first pass of Phase 3 _"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_length = int(len(df) * .2)\n",
    "valid_df = df.iloc[:valid_length]\n",
    "train_df = df.iloc[valid_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>5215.png</td>\n",
       "      <td>Speeches by the Soviet ambassador</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>3588.png</td>\n",
       "      <td>They looked up in surprise as ____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>7998.png</td>\n",
       "      <td>these are fairly large tough _____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>6149.png</td>\n",
       "      <td>You talked about JohnandLinda as _</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1404.png</td>\n",
       "      <td>Its nothing to fool with  _______</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image                            sentence\n",
       "1995  5215.png  Speeches by the Soviet ambassador \n",
       "1996  3588.png  They looked up in surprise as ____\n",
       "1997  7998.png  these are fairly large tough _____\n",
       "1998  6149.png  You talked about JohnandLinda as _\n",
       "1999  1404.png   Its nothing to fool with  _______"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>4023.png</td>\n",
       "      <td>It is sufficiently small compared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>6654.png</td>\n",
       "      <td>They played crack the whip a few _</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>6890.png</td>\n",
       "      <td>With Lizzie in the barn the ______</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>6458.png</td>\n",
       "      <td>No they must look the other way __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>2868.png</td>\n",
       "      <td>It looked as Gavin had first seen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image                            sentence\n",
       "2000  4023.png  It is sufficiently small compared \n",
       "2001  6654.png  They played crack the whip a few _\n",
       "2002  6890.png  With Lizzie in the barn the ______\n",
       "2003  6458.png  No they must look the other way __\n",
       "2004  2868.png  It looked as Gavin had first seen "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinesDataSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, df, batch_size=32):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(df) // batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a single batch.\"\"\"\n",
    "        begin = idx * self.batch_size\n",
    "        end = (idx + 1) * self.batch_size\n",
    "\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for index in range(begin, end):\n",
    "            row = self.df.iloc[index]\n",
    "            \n",
    "            image = Image.open(Dataset.processed_data_path()/'emnist_lines'/row['image'])\n",
    "            x = np.array(image).astype(np.float32).reshape(image_height, image_width, 1)\n",
    "            batch_x.append(x)\n",
    "\n",
    "            y = [mapping_reversed[char] for char in row['sentence']]\n",
    "            y = to_categorical(y, num_classes).astype(np.int)\n",
    "            batch_y.append(y)\n",
    "\n",
    "        return np.array(batch_x), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check progress in TensorBoard by running `tensorboard --logdir=logs/lines_time_distributed` in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir='../logs/lines_time_distributed')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      " 2/31 [>.............................] - ETA: 5:32 - loss: 2.9854 - accuracy: 0.2173"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d75f85b35269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     callbacks=callbacks)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    LinesDataSequence(train_df, batch_size),\n",
    "    steps_per_epoch=len(train_df) // batch_size,\n",
    "    validation_data=LinesDataSequence(valid_df, batch_size),\n",
    "    validation_steps=len(valid_df) // batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks)\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False\n",
    "\n",
    "# Compile the model for trainable changes to take effect.\n",
    "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(test_batch)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for char_pred in preds[0]:\n",
    "    print(mapping[np.argmax(char_pred)], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "line-reader",
   "language": "python",
   "name": "line-reader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
