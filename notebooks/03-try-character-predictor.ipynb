{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make that we are running the correct version of TensorFlow first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-alpha0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 6) # Python ≥3.6 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and use lenet5 for character predication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import traps: http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html\n",
    "from recognizer.datasets import EmnistDataset\n",
    "from recognizer.networks import lenet5\n",
    "from recognizer.networks import simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download path: /home/jupyter/line-reader/data/cache/datasets/matlab.zip\n",
      "Processing data...\n",
      "Balancing train dataset...\n",
      "Target max number of images per class: 21635892\n",
      "Dataset ready, with 1395864 training entries and 116323 test entries\n"
     ]
    }
   ],
   "source": [
    "emnist = EmnistDataset()\n",
    "\n",
    "train_dataset = emnist.train_dataset.shuffle(1024).batch(batch_size)# emnist.train_dataset.shuffle(1024).repeat().batch(batch_size)\n",
    "test_dataset = emnist.test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The tf.data API provides a software pipelining mechanism through the tf.data.Dataset.prefetch transformation, which can be used to decouple the time when data is produced from the time when data is consumed. In particular, the transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested. The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step. You could either manually tune this value, or set it to tf.data.experimental.AUTOTUNE which will prompt the tf.data runtime to tune the value dynamically at runtime.\n",
    "~[https://www.tensorflow.org/alpha/guide/data_performance](https://www.tensorflow.org/alpha/guide/data_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 28, 28, 1), (None, 62)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (256, 28, 28, 1), model input shape: (28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), = train_dataset.take(1)\n",
    "input_shape = x_train[0].shape\n",
    "print(f\"x shape: {x_train.shape}, model input shape: {input_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickly fit one batch and that everything is working as expected check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_test, y_test), = test_dataset.take(1)\n",
    "\n",
    "# # model = lenet5(input_shape=input_shape, number_of_classes=emnist.number_of_classes)\n",
    "# model = simple(input_shape=input_shape, number_of_classes=emnist.number_of_classes)\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x=x_train, y=y_train, epochs=5)\n",
    "# model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "[Get started with TensorFlow 2.0 for experts](https://www.tensorflow.org/alpha/tutorials/quickstart/advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoints_path = Path(\"../recognizer/ckpts/character_model\")\n",
    "model_checkpoints_path.mkdir(parents=True, exist_ok=True)\n",
    "model_save_path = Path(\"../recognizer/weights/character_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "# optimizer = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "learning_rate_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_learning_rate = 0.01\n",
    "# learning_rate_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate,\n",
    "#     decay_steps=100000,\n",
    "#     decay_rate=0.96,\n",
    "#     staircase=True)\n",
    "\n",
    "# learning_rate_schedule.ExponentialDecay(0.05, 10, 0.96)\n",
    "\n",
    "# optimizer = keras.optimizers.Adam(learning_rate=learning_rate_schedule)\n",
    "\n",
    "# for step in range(100, 100000):\n",
    "#     print(float(optimizer._decayed_lr(step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 62)                7998      \n",
      "=================================================================\n",
      "Total params: 1,206,590\n",
      "Trainable params: 1,206,590\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = lenet5(input_shape=input_shape, number_of_classes=emnist.number_of_classes)\n",
    "# model = simple(input_shape=input_shape, number_of_classes=emnist.number_of_classes)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch.\n"
     ]
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, model_checkpoints_path, max_to_keep=3)\n",
    "ckpt.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(f\"Restored from {manager.latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.88480544090271, Accuracy: 77.29872131347656, Test Loss: 0.7967246174812317, Test Accuracy: 79.8509292602539\n",
      "Epoch 2, Loss: 1.2853127717971802, Accuracy: 78.16015625, Test Loss: 0.7365587949752808, Test Accuracy: 79.0643310546875\n",
      "Epoch 3, Loss: 1.0929853916168213, Accuracy: 78.29521179199219, Test Loss: 0.7173655033111572, Test Accuracy: 78.80269622802734\n",
      "Epoch 4, Loss: 1.007307529449463, Accuracy: 78.19929504394531, Test Loss: 0.7089312076568604, Test Accuracy: 78.69424438476562\n",
      "Epoch 5, Loss: 0.9563224911689758, Accuracy: 78.08367156982422, Test Loss: 0.745503306388855, Test Accuracy: 77.70501708984375\n",
      "Epoch 6, Loss: 0.9271887540817261, Accuracy: 77.88587188720703, Test Loss: 0.7491323351860046, Test Accuracy: 77.522216796875\n",
      "Epoch 7, Loss: 0.9108269810676575, Accuracy: 77.66666412353516, Test Loss: 0.7563627362251282, Test Accuracy: 77.39864349365234\n",
      "Epoch 8, Loss: 0.8997725248336792, Accuracy: 77.4848861694336, Test Loss: 0.7983296513557434, Test Accuracy: 77.02485656738281\n",
      "💾 Saved checkpoint for step 10: ../recognizer/ckpts/character_model/ckpt-1\n",
      "Epoch 9, Loss: 0.8935125470161438, Accuracy: 77.32120513916016, Test Loss: 0.857822597026825, Test Accuracy: 76.82009887695312\n",
      "Epoch 10, Loss: 0.8912712335586548, Accuracy: 77.11741638183594, Test Loss: 0.8709021806716919, Test Accuracy: 76.21141052246094\n",
      "Epoch 11, Loss: 0.8905555009841919, Accuracy: 76.89798736572266, Test Loss: 0.8974170088768005, Test Accuracy: 75.57904815673828\n",
      "Epoch 12, Loss: 0.8907015919685364, Accuracy: 76.71537780761719, Test Loss: 0.8900039792060852, Test Accuracy: 75.67169189453125\n",
      "Epoch 13, Loss: 0.89119553565979, Accuracy: 76.5614013671875, Test Loss: 0.8854475617408752, Test Accuracy: 75.5749740600586\n",
      "Epoch 14, Loss: 0.8918948769569397, Accuracy: 76.42804718017578, Test Loss: 0.8781374096870422, Test Accuracy: 75.66449737548828\n",
      "Epoch 15, Loss: 0.9555237889289856, Accuracy: 74.75137329101562, Test Loss: 1.0654247999191284, Test Accuracy: 70.9422378540039\n",
      "Epoch 16, Loss: 1.1270242929458618, Accuracy: 70.39824676513672, Test Loss: 1.2291085720062256, Test Accuracy: 66.81024932861328\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    for images, labels in train_dataset:\n",
    "        train_step(images, labels)\n",
    "    \n",
    "    for test_images, test_labels in test_dataset:\n",
    "        test_step(test_images, test_labels)\n",
    "    \n",
    "    ckpt.step.assign_add(1)\n",
    "    if int(ckpt.step) % 10 == 0:\n",
    "        save_path = manager.save()\n",
    "        print(f\"💾 Saved checkpoint for step {int(ckpt.step)}: {save_path}\")\n",
    "        \n",
    "    print(f\"Epoch {epoch}, \"\\\n",
    "#           f\"Current learning rate: {optimizer._lr}, \"\\\n",
    "          f\"Loss: {train_loss.result()}, Accuracy: {train_accuracy.result()*100}, \"\\\n",
    "          f\"Test Loss: {test_loss.result()}, Test Accuracy: {test_accuracy.result()*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next time, start from a clean slate\n",
    "!rm -r {model_checkpoints_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "line-reader",
   "language": "python",
   "name": "line-reader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
