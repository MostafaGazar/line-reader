{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make that we are running the correct version of TensorFlow first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 6) # Python ≥3.6 required\n",
    "# assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train basic model on the generated emnist-lines dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "# To be able to write code as if this notebook was one level up in files tree structure.\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.ops import ctc_ops\n",
    "from tensorflow.python.ops import math_ops as tf_math_ops\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, Reshape, TimeDistributed, LSTM\n",
    "from tensorflow.keras.models import Model as KerasModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recognizer.datasets import Dataset\n",
    "from recognizer.datasets import EmnistDataset\n",
    "from recognizer.networks import lenet\n",
    "from recognizer.networks import simple\n",
    "from recognizer.networks import NetworkInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = Path(\"../recognizer/weights/lines_ctc_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load characters mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(Dataset.raw_data_path()/\"emnist\"/\"emnist_essentials.json\") as json_file:  \n",
    "    mapping = json.load(json_file)[\"mapping\"]\n",
    "    mapping = {m[0]: m[1] for m in mapping}\n",
    "    mapping[62] = ' '\n",
    "    mapping[63] = '_'\n",
    "    \n",
    "mapping_reversed = {v: k for k, v in mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_image_height = 28\n",
    "char_image_width = 28\n",
    "max_length = 34\n",
    "num_classes = 64\n",
    "image_height = char_image_height\n",
    "image_width = char_image_width * max_length\n",
    "\n",
    "input_shape = (image_height, image_width)\n",
    "output_shape = (max_length, num_classes)\n",
    "\n",
    "window_width: float = 16\n",
    "window_stride: float = 8\n",
    "    \n",
    "num_windows = ((image_width - window_width) // window_stride) + 1\n",
    "if num_windows < max_length:\n",
    "    raise ValueError(f'Window width/stride need to generate >= {max_length} windows (currently {num_windows})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_window(image, window_width, window_stride):\n",
    "    \"\"\"\n",
    "    Takes (image_height, image_width, 1) input,\n",
    "    Returns (num_windows, image_height, window_width, 1) output, where\n",
    "    num_windows is floor((image_width - window_width) / window_stride) + 1\n",
    "    \"\"\"\n",
    "    patches = tf.image.extract_image_patches(image, \n",
    "                                             sizes=[1, 1, window_width, 1], \n",
    "                                             strides=[1, 1, window_stride, 1], \n",
    "                                             rates=[1, 1, 1, 1], \n",
    "                                             padding='VALID')\n",
    "    patches = tf.transpose(patches, (0, 2, 1, 3))\n",
    "    patches = tf.expand_dims(patches, -1)\n",
    "    \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_width = 12\n",
    "window_stride = 5\n",
    "\n",
    "image_input = Input(shape=input_shape, name='image')\n",
    "image_reshaped = Reshape((image_height, image_width, 1))(image_input)\n",
    "\n",
    "image_patches = Lambda(\n",
    "    slide_window,\n",
    "    arguments={'window_width': window_width, 'window_stride': window_stride}\n",
    ")(image_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lambda/ExpandDims:0' shape=(?, 189, 28, 12, 1) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_decode(y_pred, input_length, max_output_length):\n",
    "    \"\"\"\n",
    "    Cut down from https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L4170\n",
    "\n",
    "    Decodes the output of a softmax.\n",
    "    Uses greedy (best path) search.\n",
    "\n",
    "    # Arguments\n",
    "        y_pred: tensor `(samples, time_steps, num_categories)`\n",
    "            containing the prediction, or output of the softmax.\n",
    "        input_length: tensor `(samples, )` containing the sequence length for\n",
    "            each batch item in `y_pred`.\n",
    "        max_output_length: int giving the max output sequence length\n",
    "\n",
    "    # Returns\n",
    "        List: list of one element that contains the decoded sequence.\n",
    "    \"\"\"\n",
    "    y_pred = tf_math_ops.log(tf.transpose(y_pred, perm=[1, 0, 2]) + K.epsilon())\n",
    "    input_length = tf.cast(tf.squeeze(input_length, axis=-1), tf.int32)\n",
    "\n",
    "    (decoded, _) = ctc_ops.ctc_greedy_decoder(inputs=y_pred, sequence_length=input_length)\n",
    "\n",
    "    sparse = decoded[0]\n",
    "#     decoded_dense = tf.sparse.to_dense(sparse.indices, sparse.dense_shape, sparse.values, default_value=-1)\n",
    "    decoded_dense = tf.sparse.to_dense(sparse, default_value=-1)\n",
    "\n",
    "    # Unfortunately, decoded_dense will be of different number of columns, depending on the decodings.\n",
    "    # We need to get it all in one standard shape, so let's pad if necessary.\n",
    "    max_length = max_output_length + 2  # giving 2 extra characters for CTC leeway\n",
    "    cols = tf.shape(decoded_dense)[-1]\n",
    "\n",
    "    def pad():\n",
    "        return tf.pad(decoded_dense, [[0, 0], [0, max_length - cols]], constant_values=-1)\n",
    "\n",
    "    def noop():\n",
    "        return decoded_dense\n",
    "\n",
    "    return tf.cond(tf.less(cols, max_length), pad, noop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0826 20:00:59.280194 4507526592 deprecation.py:506] From /Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.convolutional.Conv2D at 0x13865b1d0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1386d79b0>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1386d73c8>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x13865b588>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x1387596a0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1386d7860>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x138739e10>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x138759b38>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnet = lenet(NetworkInput(input_shape=(image_height, window_width, 1), number_of_classes=num_classes))\n",
    "convnet.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 20:00:59.853394 4507526592 deprecation.py:323] From /Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Get rid of the last two layers (dropout and softmax)\n",
    "convnet = KerasModel(inputs=convnet.inputs, outputs=convnet.layers[-2].output)\n",
    "time_distributed_outputs = TimeDistributed(convnet)(image_patches)\n",
    "\n",
    "lstm_outputs = LSTM(128, return_sequences=True)(time_distributed_outputs)\n",
    "\n",
    "softmax_output = Dense(num_classes, activation='softmax', name='softmax_output')(lstm_outputs)\n",
    "\n",
    "y_true = Input(shape=(max_length,), name='y_true')\n",
    "input_length = Input(shape=(1,), name='input_length')\n",
    "label_length = Input(shape=(1,), name='label_length')\n",
    "\n",
    "input_length_processed = Lambda(\n",
    "    lambda x, num_windows=None: x * num_windows,\n",
    "    arguments={'num_windows': num_windows}\n",
    ")(input_length)\n",
    "\n",
    "ctc_loss_output = Lambda(\n",
    "    lambda x: K.ctc_batch_cost(x[0], x[1], x[2], x[3]),\n",
    "    name='ctc_loss'\n",
    ")([y_true, softmax_output, input_length_processed, label_length])\n",
    "\n",
    "ctc_decoded_output = Lambda(\n",
    "    lambda x: ctc_decode(x[0], x[1], max_length),\n",
    "    name='ctc_decoded'\n",
    ")([softmax_output, input_length_processed])\n",
    "    \n",
    "model = KerasModel(\n",
    "        inputs=[image_input, y_true, input_length, label_length],\n",
    "        outputs=[ctc_loss_output, ctc_decoded_output]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'softmax_output/truediv:0' shape=(?, 189, 64) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lambda_1/mul:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ctc_loss/ExpandDims:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctc_loss_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ctc_decoded/cond/Merge:0' shape=(?, ?) dtype=int64>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctc_decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              [(None, 28, 952)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 28, 952, 1)   0           image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 189, 28, 12,  0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 189, 128)     412160      lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 189, 128)     131584      time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "y_true (InputLayer)             [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "softmax_output (Dense)          (None, 189, 64)      8256        lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           input_length[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc_loss (Lambda)               (None, 1)            0           y_true[0][0]                     \n",
      "                                                                 softmax_output[0][0]             \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 label_length[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ctc_decoded (Lambda)            (None, None)         0           softmax_output[0][0]             \n",
      "                                                                 lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 552,000\n",
      "Trainable params: 552,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load emnist lines and pass it to the new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Dataset.processed_data_path()/\"emnist_lines\"/\"data.csv\")\n",
    "\n",
    "output_length = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.png</td>\n",
       "      <td>Whats wrong at state _____________</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.png</td>\n",
       "      <td>The transducer itself moves the __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.png</td>\n",
       "      <td>There is an ancient and venerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.png</td>\n",
       "      <td>They destroyed a trading house ___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.png</td>\n",
       "      <td>Alec waited a moment on guard  ___</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image                            sentence\n",
       "0  0.png  Whats wrong at state _____________\n",
       "1  1.png  The transducer itself moves the __\n",
       "2  2.png  There is an ancient and venerable \n",
       "3  3.png  They destroyed a trading house ___\n",
       "4  4.png  Alec waited a moment on guard  ___"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=952x28 at 0x138F93940>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/\"0.png\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 952)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on one image before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image\n",
      "reshape\n",
      "lambda\n",
      "time_distributed\n",
      "lstm\n",
      "input_length\n",
      "y_true\n",
      "softmax_output\n",
      "lambda_1\n",
      "label_length\n",
      "ctc_loss\n",
      "ctc_decoded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(layer.name) for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dimension(189)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_output.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_length:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convnet = KerasModel(inputs=model.inputs, outputs=model.layers[-5].output)\n",
    "\n",
    "# image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/\"0.png\")\n",
    "# image_numpy = np.array(image).astype(np.float32) / 255\n",
    "# # image_numpy = image_numpy.reshape((1,) + image_numpy.shape)\n",
    "\n",
    "# # Get the prediction and confidence using softmax_output_fn, passing the right input into it.\n",
    "# input_image = np.expand_dims(image_numpy, 0)\n",
    "# softmax_output = convnet(input_image)\n",
    "\n",
    "# input_length = np.array([softmax_output.shape[1]])\n",
    "# decoded, log_prob = K.ctc_decode(softmax_output, input_length, greedy=True)\n",
    "\n",
    "# pred_raw = K.eval(decoded[0])[0]\n",
    "# pred = ''.join(mapping[label] for label in pred_raw).strip()\n",
    "\n",
    "# neg_sum_logit = K.eval(log_prob)[0][0]\n",
    "# conf = np.exp(-neg_sum_logit)\n",
    "\n",
    "# pred, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the data and reset their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6933.png</td>\n",
       "      <td>Sir People continue to inquire ___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7890.png</td>\n",
       "      <td>Even today range riders will come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1988.png</td>\n",
       "      <td>but again as one will have _______</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8800.png</td>\n",
       "      <td>Whatever the longrange impact of _</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>21.png</td>\n",
       "      <td>Despite the warning there was a __</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image                            sentence\n",
       "0  6933.png  Sir People continue to inquire ___\n",
       "1  7890.png  Even today range riders will come \n",
       "2  1988.png  but again as one will have _______\n",
       "3  8800.png  Whatever the longrange impact of _\n",
       "4    21.png  Despite the warning there was a __"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_length = int(len(df) * .2)\n",
    "valid_df = df.iloc[:valid_length]\n",
    "train_df = df.iloc[valid_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_batch_ctc(batch_x, batch_y):\n",
    "    \"\"\"\n",
    "    Because CTC loss needs to be computed inside of the network, we include information about outputs in the inputs.\n",
    "    \"\"\"\n",
    "    batch_size = batch_y.shape[0]\n",
    "    y_true = np.argmax(batch_y, axis=-1)\n",
    "    \n",
    "    label_lengths = []\n",
    "    for ind in range(batch_size):\n",
    "        # Find all of the indices in the label that are blank\n",
    "        empty_at = np.where(batch_y[ind, :, -1] == 1)[0]\n",
    "        # Length of the label is the pos of the first blank, or the max length\n",
    "        if empty_at.shape[0] > 0:\n",
    "            label_lengths.append(empty_at[0])\n",
    "        else:\n",
    "            label_lengths.append(batch_y.shape[1])\n",
    "\n",
    "    batch_inputs = {\n",
    "        'image': batch_x,\n",
    "        'y_true': y_true,\n",
    "        # TODO :: Remove the lambda layer form the model and change the size here\n",
    "        'input_length': np.ones((batch_size, 1)),  # dummy, will be set to num_windows in network\n",
    "        'label_length': np.array(label_lengths)\n",
    "    }\n",
    "    batch_outputs = {\n",
    "        'ctc_loss': np.zeros(batch_size),  # dummy\n",
    "        'ctc_decoded': y_true\n",
    "    }\n",
    "    return batch_inputs, batch_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinesDataSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, df, batch_size=32, augment_fn=None, format_fn=None):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.augment_fn = augment_fn\n",
    "        self.format_fn = format_fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) // batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a single batch.\"\"\"\n",
    "        begin = idx * self.batch_size\n",
    "        end = (idx + 1) * self.batch_size\n",
    "\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for index in range(begin, end):\n",
    "            row = self.df.iloc[index]\n",
    "            \n",
    "            image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/row['image'])\n",
    "            x = np.array(image).astype(np.float32).reshape(image_height, image_width)\n",
    "            batch_x.append(x)\n",
    "\n",
    "            y = [mapping_reversed[char] for char in row['sentence']]\n",
    "            y = to_categorical(y, num_classes).astype(np.int)\n",
    "            batch_y.append(y)\n",
    "            \n",
    "        batch_x, batch_y = np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "        return format_batch_ctc(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss():\n",
    "    \"\"\"Dummy loss function: just pass through the loss that we computed in the network.\"\"\"\n",
    "    return {'ctc_loss': lambda y_true, y_pred: y_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check progress in TensorBoard by running `tensorboard --logdir=logs/lines_time_distributed_ctc` in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir='../logs/lines_time_distributed_ctc')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 20:01:00.615096 4507526592 training_utils.py:1101] Output ctc_decoded missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to ctc_decoded.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=RMSprop(), loss=loss(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "  7/625 [..............................] - ETA: 12:37 - loss: 170.2109 - ctc_loss_loss: 170.2109 - ctc_loss_acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-014f5af2b60f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    LinesDataSequence(train_df, batch_size),\n",
    "    steps_per_epoch=len(df) // batch_size,\n",
    "    validation_data=LinesDataSequence(valid_df, batch_size),\n",
    "    validation_steps=len(valid_df) // batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(model_save_path)\n",
    "\n",
    "model.trainable = False\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss=loss(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet = KerasModel(inputs=model.inputs, outputs=model.layers[-5].output)\n",
    "\n",
    "image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/\"0.png\")\n",
    "image_numpy = np.array(image).astype(np.float32) / 255\n",
    "\n",
    "# Get the prediction and confidence using softmax_output_fn, passing the right input into it.\n",
    "input_image = np.expand_dims(image_numpy, 0)\n",
    "softmax_output = convnet(input_image)\n",
    "\n",
    "input_length = np.array([softmax_output.shape[1]])\n",
    "decoded, log_prob = K.ctc_decode(softmax_output, input_length, greedy=True)\n",
    "\n",
    "pred_raw = K.eval(decoded[0])[0]\n",
    "pred = ''.join(mapping[label] for label in pred_raw).strip()\n",
    "\n",
    "neg_sum_logit = K.eval(log_prob)[0][0]\n",
    "conf = np.exp(-neg_sum_logit)\n",
    "\n",
    "pred, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "line-reader",
   "language": "python",
   "name": "line-reader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
