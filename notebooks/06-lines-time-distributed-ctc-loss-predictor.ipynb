{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make that we are running the correct version of TensorFlow first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 6) # Python ≥3.6 required\n",
    "# assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN model with CTC loss on the generated emnist-lines dataset\n",
    "\n",
    "From Keras examples [image_ocr.py](https://github.com/keras-team/keras/blob/master/examples/image_ocr.py) and [Chengwei's post](https://www.dlology.com/blog/how-to-train-a-keras-model-to-recognize-variable-length-text/) helped me a lot in getting the ctc loss working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "# To be able to write code as if this notebook was one level up in files tree structure.\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.ops import ctc_ops\n",
    "from tensorflow.python.ops import math_ops as tf_math_ops\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, Reshape, TimeDistributed, LSTM\n",
    "from tensorflow.keras.models import Model as KerasModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recognizer.datasets import Dataset\n",
    "from recognizer.datasets import EmnistDataset\n",
    "from recognizer.networks import lenet\n",
    "from recognizer.networks import simple\n",
    "from recognizer.networks import NetworkInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = Path(\"../recognizer/weights/lines_ctc_loss.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load characters mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(Dataset.raw_data_path()/\"emnist\"/\"emnist_essentials.json\") as json_file:  \n",
    "    mapping = json.load(json_file)[\"mapping\"]\n",
    "    mapping = {m[0]: m[1] for m in mapping}\n",
    "    mapping[62] = ' '\n",
    "    mapping[63] = '_'\n",
    "    \n",
    "mapping_reversed = {v: k for k, v in mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_image_height = 28\n",
    "char_image_width = 28\n",
    "max_length = 34\n",
    "num_classes = 64\n",
    "image_height = char_image_height\n",
    "image_width = char_image_width * max_length\n",
    "\n",
    "input_shape = (image_height, image_width)\n",
    "output_shape = (max_length, num_classes)\n",
    "\n",
    "window_width: float = 16\n",
    "window_stride: float = 8\n",
    "    \n",
    "num_windows = ((image_width - window_width) // window_stride) + 1\n",
    "if num_windows < max_length:\n",
    "    raise ValueError(f'Window width/stride need to generate >= {max_length} windows (currently {num_windows})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_window(image, window_width, window_stride):\n",
    "    \"\"\"\n",
    "    Takes (image_height, image_width, 1) input,\n",
    "    Returns (num_windows, image_height, window_width, 1) output, where\n",
    "    num_windows is floor((image_width - window_width) / window_stride) + 1\n",
    "    \"\"\"\n",
    "    patches = tf.image.extract_image_patches(image, \n",
    "                                             sizes=[1, 1, window_width, 1], \n",
    "                                             strides=[1, 1, window_stride, 1], \n",
    "                                             rates=[1, 1, 1, 1], \n",
    "                                             padding='VALID')\n",
    "    patches = tf.transpose(patches, (0, 2, 1, 3))\n",
    "    patches = tf.expand_dims(patches, -1)\n",
    "    \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_width = 12\n",
    "window_stride = 5\n",
    "\n",
    "image_input = Input(shape=input_shape, name='image')\n",
    "image_reshaped = Reshape((image_height, image_width, 1))(image_input)\n",
    "\n",
    "image_patches = Lambda(\n",
    "    slide_window,\n",
    "    arguments={'window_width': window_width, 'window_stride': window_stride}\n",
    ")(image_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lambda/ExpandDims:0' shape=(?, 189, 28, 12, 1) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the actual loss calc occurs here despite it not being an internal Keras loss function\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0828 08:48:28.963287 4412421568 deprecation.py:506] From /Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.convolutional.Conv2D at 0x1345456a0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x13457c240>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x13457c080>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x1345ba320>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x1345bac18>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x13457c208>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x134582710>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x13464a9b0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnet_base = lenet(NetworkInput(input_shape=(image_height, window_width, 1), number_of_classes=num_classes))\n",
    "convnet_base.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0828 08:48:29.528858 4412421568 deprecation.py:323] From /Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image (InputLayer)           [(None, 28, 952)]         0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 952, 1)        0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 189, 28, 12, 1)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 189, 128)          412160    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 189, 128)          131584    \n",
      "_________________________________________________________________\n",
      "softmax_output (Dense)       (None, 189, 64)           8256      \n",
      "=================================================================\n",
      "Total params: 552,000\n",
      "Trainable params: 552,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              [(None, 28, 952)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 28, 952, 1)   0           image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 189, 28, 12,  0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 189, 128)     412160      lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 189, 128)     131584      time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "softmax_output (Dense)          (None, 189, 64)      8256        lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax_output[0][0]             \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 552,000\n",
      "Trainable params: 552,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get rid of the last two layers (dropout and softmax)\n",
    "convnet = KerasModel(inputs=convnet_base.inputs, outputs=convnet_base.layers[-2].output)\n",
    "time_distributed_outputs = TimeDistributed(convnet)(image_patches)\n",
    "\n",
    "lstm_outputs = LSTM(128, return_sequences=True)(time_distributed_outputs)\n",
    "\n",
    "y_pred = Dense(num_classes, activation='softmax', name='softmax_output')(lstm_outputs)\n",
    "KerasModel(inputs=image_input, outputs=y_pred).summary()\n",
    "\n",
    "# Add ctc specific ipnuts for the training model, the predication model will just need access to `image_input`\n",
    "labels = Input(name='the_labels', shape=[max_length], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "# Keras doesn't currently support loss funcs with extra parameters\n",
    "# so CTC loss is implemented in a lambda layer\n",
    "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "\n",
    "# clipnorm seems to speeds up convergence\n",
    "sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "\n",
    "model = KerasModel(inputs=[image_input, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "model.compile(optimizer=sgd, loss={'ctc': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Dataset.processed_data_path()/\"emnist_lines\"/\"data.csv\")\n",
    "\n",
    "output_length = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.png</td>\n",
       "      <td>Whats wrong at state _____________</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.png</td>\n",
       "      <td>The transducer itself moves the __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.png</td>\n",
       "      <td>There is an ancient and venerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.png</td>\n",
       "      <td>They destroyed a trading house ___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.png</td>\n",
       "      <td>Alec waited a moment on guard  ___</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image                            sentence\n",
       "0  0.png  Whats wrong at state _____________\n",
       "1  1.png  The transducer itself moves the __\n",
       "2  2.png  There is an ancient and venerable \n",
       "3  3.png  They destroyed a trading house ___\n",
       "4  4.png  Alec waited a moment on guard  ___"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the data and reset their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1876.png</td>\n",
       "      <td>His proposal is opposed to that __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5824.png</td>\n",
       "      <td>According to his own testimony he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7483.png</td>\n",
       "      <td>The anode plug Figure 2 was ______</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4937.png</td>\n",
       "      <td>Hundreds of civic clubs business _</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8177.png</td>\n",
       "      <td>The resident staff is large and __</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image                            sentence\n",
       "0  1876.png  His proposal is opposed to that __\n",
       "1  5824.png  According to his own testimony he \n",
       "2  7483.png  The anode plug Figure 2 was ______\n",
       "3  4937.png  Hundreds of civic clubs business _\n",
       "4  8177.png  The resident staff is large and __"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For the sake of debugging let us test only one sentence\n",
    "# df = df.iloc[[0] * len(df)]\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_length = int(len(df) * .2)\n",
    "\n",
    "train_df = df.iloc[valid_length:]\n",
    "valid_df = df.iloc[:valid_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinesDataSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, df, batch_size=32):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) // batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a single batch.\"\"\"\n",
    "        X_data = np.ones([self.batch_size, image_height, image_width])\n",
    "\n",
    "        labels = np.ones([self.batch_size, max_length])\n",
    "        input_length = np.zeros([self.batch_size, 1])\n",
    "        label_length = np.zeros([self.batch_size, 1])\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            index = i + idx\n",
    "            row = self.df.iloc[index]\n",
    "            \n",
    "            image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/row['image'])\n",
    "            image = np.array(image).astype(np.float32).reshape(image_height, image_width)\n",
    "            X_data[i, :, :] = image\n",
    "            \n",
    "            y = [mapping_reversed[char] for char in row['sentence']]\n",
    "            y = to_categorical(y, num_classes).astype(np.int)\n",
    "#             labels[i, :] = y\n",
    "            labels[i, :] = np.argmax(y, axis=-1)\n",
    "#             labels[i, :] = np.asarray(y)\n",
    "            \n",
    "            # TODO :: Not sure what to do with this!\n",
    "            input_length[i] = 64  # 34  # 189\n",
    "            \n",
    "            # Find all of the indices in the label that are not blank\n",
    "            empty_at = np.where(y[:, -1] == 1)[0]\n",
    "            # Length of the label is the pos of the first blank, or the max length\n",
    "            if empty_at.shape[0] > 0:\n",
    "                label_length[i] = empty_at[0]\n",
    "            else:\n",
    "                label_length[i] = y.shape[0]\n",
    "            \n",
    "        inputs = {\n",
    "            'image': X_data,\n",
    "            'the_labels': labels,\n",
    "            'input_length': input_length,\n",
    "            'label_length': label_length\n",
    "        }\n",
    "        outputs = {'ctc': np.zeros([self.batch_size])}  # dummy data for dummy loss function\n",
    "\n",
    "        return (inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check progress in TensorBoard by running `tensorboard --logdir=logs/lines_ctc_loss` in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir='../logs/lines_ctc_loss')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 700s 1s/step - loss: inf - acc: 0.0000e+00 - val_loss: inf - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 683s 1s/step - loss: inf - acc: 0.0000e+00 - val_loss: inf - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      " 62/625 [=>............................] - ETA: 10:13 - loss: inf - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8f5c8396c497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     callbacks=callbacks)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    LinesDataSequence(train_df, batch_size),\n",
    "    steps_per_epoch=len(df) // batch_size,\n",
    "    validation_data=LinesDataSequence(valid_df, batch_size),\n",
    "    validation_steps=len(valid_df) // batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks)\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6815abebb2e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(model_save_path)\n",
    "\n",
    "# model.trainable = False\n",
    "# model.compile(optimizer=sgd, loss={'ctc': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "# Load weights into the model instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image (InputLayer)           [(None, 28, 952)]         0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 952, 1)        0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 189, 28, 12, 1)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 189, 128)          412160    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 189, 128)          131584    \n",
      "_________________________________________________________________\n",
      "softmax_output (Dense)       (None, 189, 64)           8256      \n",
      "=================================================================\n",
      "Total params: 552,000\n",
      "Trainable params: 552,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_pred = KerasModel(inputs=image_input, outputs=y_pred)\n",
    "# model.trainable = False\n",
    "# model.compile(optimizer=sgd, loss={'ctc': lambda y_true, y_pred: y_pred}, metrics=['accuracy'])\n",
    "# model.load_weights(weight_file)\n",
    "\n",
    "model_pred.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_text(labels):\n",
    "    return \"\".join([mapping[c] for c in labels])\n",
    "\n",
    "def decode_predict_ctc(out, top_paths=1):\n",
    "    results = []\n",
    "    beam_width = 5\n",
    "    if beam_width < top_paths:\n",
    "          beam_width = top_paths\n",
    "    for i in range(top_paths):\n",
    "        lables = K.get_value(K.ctc_decode(out, input_length=np.ones(out.shape[0])*out.shape[1],\n",
    "                           greedy=False, beam_width=beam_width, top_paths=top_paths)[0][i])[0]\n",
    "        text = labels_to_text(lables)\n",
    "        results.append(text)\n",
    "        \n",
    "    return results\n",
    "  \n",
    "def predit_on_image(model, img, top_paths=1):\n",
    "    batch = np.expand_dims(img, axis=0)  # Create a fake batch of one image\n",
    "    net_out_value = model.predict(batch)\n",
    "    top_pred_texts = decode_predict_ctc(net_out_value, top_paths)\n",
    "    \n",
    "    return top_pred_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Th e e e e e e e e e e e e t e e e e e e t e e t e e t ',\n",
       " 'Th e e e e e e e e e e e e t e e e e e e t e e t e e e t ',\n",
       " 'Th e e e e e e e e e e e e t e e e e e e t e e t e e ']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/df.iloc[index]['image'])\n",
    "image_numpy = np.array(image).astype(np.float32) / 255\n",
    "\n",
    "# sentence = df.iloc[index]['sentence']\n",
    "# y = [mapping_reversed[char] for char in sentence]\n",
    "# y = to_categorical(y, num_classes).astype(np.int)\n",
    "# test_batch_y = y.reshape((1,) + y.shape)\n",
    "\n",
    "results = predit_on_image(model_pred, image_numpy, top_paths=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=952x28 at 0x13538BBA8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "line-reader",
   "language": "python",
   "name": "line-reader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
