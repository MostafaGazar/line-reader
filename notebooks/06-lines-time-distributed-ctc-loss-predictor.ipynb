{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make that we are running the correct version of TensorFlow first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 6) # Python ≥3.6 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN model with CTC loss on the generated emnist-lines dataset\n",
    "\n",
    "From Keras examples [image_ocr.py](https://github.com/keras-team/keras/blob/master/examples/image_ocr.py) and [Chengwei's post](https://www.dlology.com/blog/how-to-train-a-keras-model-to-recognize-variable-length-text/) helped me a lot in getting the ctc loss working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "# To be able to write code as if this notebook was one level up in files tree structure.\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.ops import ctc_ops\n",
    "from tensorflow.python.ops import math_ops as tf_math_ops\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model as KerasModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recognizer.datasets import Dataset\n",
    "from recognizer.datasets import EmnistDataset\n",
    "from recognizer.networks import lenet\n",
    "from recognizer.networks import simple\n",
    "from recognizer.networks import NetworkInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 16\n",
    "train_valid_length = 10000  # 124  # 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = Path(\"../recognizer/weights/lines_ctc_loss.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load characters mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(Dataset.raw_data_path()/\"emnist\"/\"emnist_essentials.json\") as json_file:  \n",
    "    mapping = json.load(json_file)[\"mapping\"]\n",
    "    mapping = {m[0]: m[1] for m in mapping}\n",
    "    mapping[62] = ' '\n",
    "    mapping[63] = '_'\n",
    "    \n",
    "mapping_reversed = {v: k for k, v in mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_text(labels):\n",
    "    return \"\".join([mapping[c] for c in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_image_height = 28\n",
    "char_image_width = 28\n",
    "max_length = 34\n",
    "num_classes = 64\n",
    "image_height = char_image_height\n",
    "image_width = char_image_width * max_length\n",
    "\n",
    "input_shape = (image_height, image_width)\n",
    "output_shape = (max_length, num_classes)\n",
    "\n",
    "window_width: float = 16\n",
    "window_stride: float = 8\n",
    "    \n",
    "num_windows = ((image_width - window_width) // window_stride) + 1\n",
    "if num_windows < max_length:\n",
    "    raise ValueError(f'Window width/stride need to generate >= {max_length} windows (currently {num_windows})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_window(image, window_width, window_stride):\n",
    "    \"\"\"\n",
    "    Takes (image_height, image_width, 1) input,\n",
    "    Returns (num_windows, image_height, window_width, 1) output, where\n",
    "    num_windows is floor((image_width - window_width) / window_stride) + 1\n",
    "    \"\"\"\n",
    "    patches = tf.image.extract_patches(image, \n",
    "                                             sizes=[1, 1, window_width, 1], \n",
    "                                             strides=[1, 1, window_stride, 1], \n",
    "                                             rates=[1, 1, 1, 1], \n",
    "                                             padding='VALID')\n",
    "    patches = tf.transpose(patches, (0, 2, 1, 3))\n",
    "    patches = tf.expand_dims(patches, -1)\n",
    "    \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_width = 12\n",
    "window_stride = 5\n",
    "\n",
    "image_input = layers.Input(shape=input_shape, name='image')\n",
    "image_reshaped = layers.Reshape((image_height, image_width, 1))(image_input)\n",
    "\n",
    "image_patches = layers.Lambda(\n",
    "    slide_window,\n",
    "    arguments={'window_width': window_width, 'window_stride': window_stride}\n",
    ")(image_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lambda/Identity:0' shape=(None, 189, 28, 12, 1) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the actual loss calc occurs here despite it not being an internal Keras loss function\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "# For merging the windows and getting the predicted text in readable form\n",
    "def decode_predict_ctc(outs, top_paths=1):\n",
    "    results = []\n",
    "    beam_width = 5\n",
    "    if beam_width < top_paths:\n",
    "          beam_width = top_paths\n",
    "  \n",
    "    for out in outs:\n",
    "        out = np.expand_dims(out, axis=0)\n",
    "        paths = []\n",
    "        for i in range(top_paths):\n",
    "            lables = K.get_value(K.ctc_decode(out, input_length=np.ones(out.shape[0])*out.shape[1],\n",
    "                               greedy=False, beam_width=beam_width, top_paths=top_paths)[0][i])[0]\n",
    "            text = labels_to_text(lables)\n",
    "            paths.append(text)\n",
    "\n",
    "        results.append(paths)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fede0e5e3d0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fede0e69990>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fede0584b50>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7fede0070cd0>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x7fede00707d0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fee31a74b10>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7fee31b99710>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fede0599ed0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnet_base = lenet(NetworkInput(input_shape=(image_height, window_width, 1), number_of_classes=num_classes))\n",
    "convnet_base.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image (InputLayer)           [(None, 28, 952)]         0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 952, 1)        0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 189, 28, 12, 1)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 189, 128)          412160    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 189, 128)          131584    \n",
      "_________________________________________________________________\n",
      "softmax_output (Dense)       (None, 189, 64)           8256      \n",
      "=================================================================\n",
      "Total params: 552,000\n",
      "Trainable params: 552,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1340: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              [(None, 28, 952)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 28, 952, 1)   0           image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 189, 28, 12,  0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 189, 128)     412160      lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 189, 128)     131584      time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "softmax_output (Dense)          (None, 189, 64)      8256        lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax_output[0][0]             \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 552,000\n",
      "Trainable params: 552,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get rid of the last two layers (dropout and softmax)\n",
    "convnet = KerasModel(inputs=convnet_base.inputs, outputs=convnet_base.layers[-2].output)\n",
    "time_distributed_outputs = layers.TimeDistributed(convnet)(image_patches)\n",
    "\n",
    "# Try a single lstm\n",
    "rnn_outputs = layers.LSTM(128, return_sequences=True)(time_distributed_outputs)\n",
    "\n",
    "# Try one GRU layer\n",
    "# rnn_outputs = layers.GRU(256, return_sequences=True)(time_distributed_outputs)\n",
    "\n",
    "# Try two layers of bidirectional GRUs\n",
    "# rnn_outputs = layers.Bidirectional(layers.GRU(128, return_sequences=True))(time_distributed_outputs)\n",
    "# rnn_outputs = layers.Bidirectional(layers.GRU(64, return_sequences=True))(rnn_outputs)\n",
    "\n",
    "y_pred = layers.Dense(num_classes, activation='softmax', name='softmax_output')(rnn_outputs)\n",
    "KerasModel(inputs=image_input, outputs=y_pred).summary()\n",
    "\n",
    "# Add ctc specific ipnuts for the training model, the predication model will just need access to `image_input`\n",
    "labels = layers.Input(name='the_labels', shape=[max_length], dtype='float32')\n",
    "input_length = layers.Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = layers.Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "# Keras doesn't currently support loss funcs with extra parameters\n",
    "# so CTC loss is implemented in a lambda layer\n",
    "loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "\n",
    "model = KerasModel(inputs=[image_input, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "# # clipnorm seems to speeds up convergence\n",
    "sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "model.compile(optimizer=sgd, loss={'ctc': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Dataset.processed_data_path()/\"emnist_lines\"/\"data.csv\")\n",
    "\n",
    "output_length = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.png</td>\n",
       "      <td>Whats wrong at state _____________</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.png</td>\n",
       "      <td>The transducer itself moves the __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.png</td>\n",
       "      <td>There is an ancient and venerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.png</td>\n",
       "      <td>They destroyed a trading house ___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.png</td>\n",
       "      <td>Alec waited a moment on guard  ___</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image                            sentence\n",
       "0  0.png  Whats wrong at state _____________\n",
       "1  1.png  The transducer itself moves the __\n",
       "2  2.png  There is an ancient and venerable \n",
       "3  3.png  They destroyed a trading house ___\n",
       "4  4.png  Alec waited a moment on guard  ___"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the data and reset their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>20433.png</td>\n",
       "      <td>Lucy Upton was graduated from the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1603.png</td>\n",
       "      <td>If you cant think of a thing to __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1067.png</td>\n",
       "      <td>The number of people acting as ___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>34622.png</td>\n",
       "      <td>_________________________________</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1034.png</td>\n",
       "      <td>Far from being irrelevant to the _</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image                            sentence\n",
       "0  20433.png  Lucy Upton was graduated from the \n",
       "1   1603.png  If you cant think of a thing to __\n",
       "2   1067.png  The number of people acting as ___\n",
       "3  34622.png   _________________________________\n",
       "4   1034.png  Far from being irrelevant to the _"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For the sake of debugging let us test only one sentence\n",
    "# df = df.iloc[[0] * len(df)]\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[:train_valid_length]\n",
    "\n",
    "valid_length = int(len(df) * .2)\n",
    "\n",
    "train_df = df.iloc[valid_length:]\n",
    "valid_df = df.iloc[:valid_length]\n",
    "\n",
    "len(train_df), valid_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinesDataSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, df, batch_size=32):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) // batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a single batch.\"\"\"\n",
    "        X_data = np.ones([self.batch_size, image_height, image_width])\n",
    "\n",
    "        labels = np.ones([self.batch_size, max_length])\n",
    "        input_length = np.zeros([self.batch_size, 1])\n",
    "        label_length = np.zeros([self.batch_size, 1])\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            index = i + idx\n",
    "            row = self.df.iloc[index]\n",
    "            \n",
    "            image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/row['image'])\n",
    "            image = np.array(image).astype(np.float32) / 255\n",
    "            image = image.reshape(image_height, image_width)\n",
    "            X_data[i, :, :] = image\n",
    "            \n",
    "            y = [mapping_reversed[char] for char in row['sentence']]\n",
    "            y = to_categorical(y, num_classes).astype(np.int)\n",
    "            labels[i, :] = np.argmax(y, axis=-1)\n",
    "            \n",
    "            # input_length refers to your sequence length and label_length refers to the ground truth label length\n",
    "            # TODO :: Not sure what to do with this!\n",
    "            input_length[i] = 189 - 2  # 64  # 34  # 189\n",
    "            \n",
    "            # Find all of the indices in the label that are not blank\n",
    "            empty_at = np.where(y[:, -1] == 1)[0]\n",
    "            # Length of the label is the pos of the first blank, or the max length\n",
    "            if empty_at.shape[0] > 0:\n",
    "                label_length[i] = empty_at[0]\n",
    "            else:\n",
    "                label_length[i] = y.shape[0]\n",
    "            \n",
    "        inputs = {\n",
    "            'image': X_data,\n",
    "            'the_labels': labels,\n",
    "            'input_length': input_length,\n",
    "            'label_length': label_length\n",
    "        }\n",
    "        outputs = {'ctc': np.zeros([self.batch_size])}  # dummy data for dummy loss function\n",
    "\n",
    "        return (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = LinesDataSequence(train_df, batch_size)\n",
    "steps_per_epoch = len(train_df) // batch_size\n",
    "validation_sequence = LinesDataSequence(valid_df, batch_size)\n",
    "validation_steps = len(valid_df) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 0\n",
    "\n",
    "# image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/df.iloc[index]['image'])\n",
    "# image_numpy = np.array(image).astype(np.float32)\n",
    "\n",
    "# sentence = df.iloc[index]['sentence']\n",
    "\n",
    "# batch = np.expand_dims(image_numpy, axis=0)  # Create a fake batch of one image\n",
    "\n",
    "\n",
    "# model_pred = KerasModel(inputs=image_input, outputs=y_pred)\n",
    "\n",
    "# net_out_values = model_pred.predict(batch)\n",
    "# text_pred = decode_predict_ctc(net_out_values, top_paths=1)\n",
    "# sentence, text_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "\n",
    "\n",
    "class ValidationDistanceCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        mean_distance = 0.\n",
    "        \n",
    "        model_pred = KerasModel(inputs=image_input, outputs=y_pred)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = model.evaluate_generator(\n",
    "            validation_sequence,\n",
    "            steps=validation_steps\n",
    "        )\n",
    "        \n",
    "        # Get the predications\n",
    "        net_out_values = model_pred.predict_generator(\n",
    "            validation_sequence,\n",
    "            steps=validation_steps\n",
    "        )\n",
    "        text_pred = decode_predict_ctc(net_out_values, top_paths=1)\n",
    "        \n",
    "        index = 0\n",
    "        print_first = True\n",
    "        for sequence, _ in validation_sequence:\n",
    "            for labels in sequence['the_labels']:\n",
    "                text = labels_to_text(labels)\n",
    "                if print_first:\n",
    "                    print(f\"\\nTrue: {text}, pred: {text_pred[index][0]}\")\n",
    "                    print_first = False\n",
    "                mean_distance += editdistance.eval(text_pred[index][0], text)\n",
    "                index += 1\n",
    "\n",
    "        mean_distance /= index\n",
    "        \n",
    "        print(f\"\\nEvaluating: total {index}, loss {loss:.4f}, mean_distance {mean_distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check progress in TensorBoard by running `tensorboard --logdir=logs/lines_ctc_loss` in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir='../logs/lines_ctc_loss'),\n",
    "    ValidationDistanceCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "61/62 [============================>.] - ETA: 1s - loss: 115.0228WARNING:tensorflow:From /home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:5537: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "\n",
      "True: Lucy Upton was graduated from the , pred: [' ']\n",
      "\n",
      "Evaluating: total 1920, loss 94.1429, mean_distance 33.0000\n",
      "62/62 [==============================] - 96s 2s/step - loss: 114.6956\n",
      "Epoch 2/16\n",
      "61/62 [============================>.] - ETA: 0s - loss: 93.2407\n",
      "True: Lucy Upton was graduated from the , pred: [' ']\n",
      "\n",
      "Evaluating: total 1920, loss 93.7660, mean_distance 33.0000\n",
      "62/62 [==============================] - 83s 1s/step - loss: 93.2580\n",
      "Epoch 3/16\n",
      "61/62 [============================>.] - ETA: 0s - loss: 92.6457\n",
      "True: Lucy Upton was graduated from the , pred: [' ']\n",
      "\n",
      "Evaluating: total 1920, loss 93.8295, mean_distance 33.0000\n",
      "62/62 [==============================] - 83s 1s/step - loss: 92.6741\n",
      "Epoch 4/16\n",
      "61/62 [============================>.] - ETA: 0s - loss: 92.1516\n",
      "True: Lucy Upton was graduated from the , pred: [' ']\n",
      "\n",
      "Evaluating: total 1920, loss 92.9369, mean_distance 33.0000\n",
      "62/62 [==============================] - 83s 1s/step - loss: 92.1387\n",
      "Epoch 5/16\n",
      "61/62 [============================>.] - ETA: 0s - loss: 90.1256\n",
      "True: Lucy Upton was graduated from the , pred: ['e']\n",
      "\n",
      "Evaluating: total 1920, loss 89.3100, mean_distance 33.0078\n",
      "62/62 [==============================] - 82s 1s/step - loss: 90.0872\n",
      "Epoch 6/16\n",
      "61/62 [============================>.] - ETA: 0s - loss: 83.3906\n",
      "True: Lucy Upton was graduated from the , pred: ['e ']\n",
      "\n",
      "Evaluating: total 1920, loss 80.1407, mean_distance 32.6245\n",
      "62/62 [==============================] - 83s 1s/step - loss: 83.3130\n",
      "Epoch 7/16\n",
      "61/62 [============================>.] - ETA: 0s - loss: 73.7296\n",
      "True: Lucy Upton was graduated from the , pred: [' re r ']\n",
      "\n",
      "Evaluating: total 1920, loss 66.2878, mean_distance 28.7693\n",
      "62/62 [==============================] - 83s 1s/step - loss: 73.6021\n",
      "Epoch 8/16\n",
      "61/62 [============================>.] - ETA: 0s - loss: 55.8278\n",
      "True: Lucy Upton was graduated from the , pred: ['e n a rote ra th ']\n",
      "\n",
      "Evaluating: total 1920, loss 46.6915, mean_distance 17.9135\n",
      "62/62 [==============================] - 84s 1s/step - loss: 55.6572\n",
      "Epoch 9/16\n",
      "61/62 [============================>.] - ETA: 0s - loss: 37.8558\n",
      "True: Lucy Upton was graduated from the , pred: ['wey ufhun as rauate fra the ']\n",
      "\n",
      "Evaluating: total 1920, loss 33.3898, mean_distance 13.3932\n",
      "62/62 [==============================] - 85s 1s/step - loss: 37.7456\n",
      "Epoch 10/16\n",
      "61/62 [============================>.] - ETA: 0s - loss: 24.7804\n",
      "True: Lucy Upton was graduated from the , pred: ['wey uphon was grauated tfro the ']\n",
      "\n",
      "Evaluating: total 1920, loss 26.3843, mean_distance 10.9391\n",
      "62/62 [==============================] - 85s 1s/step - loss: 24.6764\n",
      "Epoch 11/16\n",
      "61/62 [============================>.] - ETA: 0s - loss: 14.7471\n",
      "True: Lucy Upton was graduated from the , pred: ['Wwy upon was grabuated tfro the ']\n",
      "\n",
      "Evaluating: total 1920, loss 22.7363, mean_distance 10.0552\n",
      "62/62 [==============================] - 85s 1s/step - loss: 14.6896\n",
      "Epoch 12/16\n",
      "61/62 [============================>.] - ETA: 0s - loss: 8.0332\n",
      "True: Lucy Upton was graduated from the , pred: ['Wwey upon was grauated tfrom the ']\n",
      "\n",
      "Evaluating: total 1920, loss 21.8402, mean_distance 9.4974\n",
      "62/62 [==============================] - 84s 1s/step - loss: 8.0098\n",
      "Epoch 13/16\n",
      "61/62 [============================>.] - ETA: 0s - loss: 4.4490\n",
      "True: Lucy Upton was graduated from the , pred: ['Wucy ufon was grauated tfrom the ']\n",
      "\n",
      "Evaluating: total 1920, loss 22.5825, mean_distance 9.2917\n",
      "62/62 [==============================] - 84s 1s/step - loss: 4.4270\n",
      "Epoch 14/16\n",
      "61/62 [============================>.] - ETA: 0s - loss: 2.7312\n",
      "True: Lucy Upton was graduated from the , pred: ['Wucy upton was grauated tfrom the ']\n",
      "\n",
      "Evaluating: total 1920, loss 22.6039, mean_distance 9.0922\n",
      "62/62 [==============================] - 84s 1s/step - loss: 2.7255\n",
      "Epoch 15/16\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.8357\n",
      "True: Lucy Upton was graduated from the , pred: ['Wucy upton was grauated tfrom the ']\n",
      "\n",
      "Evaluating: total 1920, loss 22.9891, mean_distance 8.9885\n",
      "62/62 [==============================] - 84s 1s/step - loss: 1.8262\n",
      "Epoch 16/16\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.3405\n",
      "True: Lucy Upton was graduated from the , pred: ['Wucy upon was grauated tfrom the ']\n",
      "\n",
      "Evaluating: total 1920, loss 22.6930, mean_distance 8.9729\n",
      "62/62 [==============================] - 83s 1s/step - loss: 1.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_sequence,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "#     validation_data=validation_sequence,\n",
    "#     validation_steps=validation_steps,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks)\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-6815abebb2e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(model_save_path)\n",
    "\n",
    "# model.trainable = False\n",
    "# model.compile(optimizer=sgd, loss={'ctc': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "# Load weights into the model instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred = KerasModel(inputs=image_input, outputs=y_pred)\n",
    "# model_pred.load_weights(str(model_save_path))\n",
    "# model_pred.trainable = False\n",
    "# model_pred.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "\n",
    "model_pred.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predit_on_image(model, img, top_paths=1):\n",
    "    batch = np.expand_dims(img, axis=0)  # Create a fake batch of one image\n",
    "    net_out_value = model.predict(batch)\n",
    "    top_pred_texts = decode_predict_ctc(net_out_value, top_paths)\n",
    "    \n",
    "    return top_pred_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/df.iloc[index]['image'])\n",
    "image_numpy = np.array(image).astype(np.float32) / 255\n",
    "\n",
    "sentence = df.iloc[index]['sentence']\n",
    "# y = [mapping_reversed[char] for char in sentence]\n",
    "# y = to_categorical(y, num_classes).astype(np.int)\n",
    "# test_batch_y = y.reshape((1,) + y.shape)\n",
    "\n",
    "results = predit_on_image(model_pred, image_numpy, top_paths=3)\n",
    "sentence, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "line-reader",
   "language": "python",
   "name": "line-reader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
