{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make that we are running the correct version of TensorFlow first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 6) # Python ≥3.6 required\n",
    "# assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN model with CTC loss on the generated emnist-lines dataset\n",
    "\n",
    "From Keras examples [image_ocr.py](https://github.com/keras-team/keras/blob/master/examples/image_ocr.py) and [Chengwei's post](https://www.dlology.com/blog/how-to-train-a-keras-model-to-recognize-variable-length-text/) helped me a lot in getting the ctc loss working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "# To be able to write code as if this notebook was one level up in files tree structure.\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.ops import ctc_ops\n",
    "from tensorflow.python.ops import math_ops as tf_math_ops\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model as KerasModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recognizer.datasets import Dataset\n",
    "from recognizer.datasets import EmnistDataset\n",
    "from recognizer.networks import lenet\n",
    "from recognizer.networks import simple\n",
    "from recognizer.networks import NetworkInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = Path(\"../recognizer/weights/lines_ctc_loss.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load characters mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(Dataset.raw_data_path()/\"emnist\"/\"emnist_essentials.json\") as json_file:  \n",
    "    mapping = json.load(json_file)[\"mapping\"]\n",
    "    mapping = {m[0]: m[1] for m in mapping}\n",
    "    mapping[62] = ' '\n",
    "    mapping[63] = '_'\n",
    "    \n",
    "mapping_reversed = {v: k for k, v in mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_image_height = 28\n",
    "char_image_width = 28\n",
    "max_length = 34\n",
    "num_classes = 64\n",
    "image_height = char_image_height\n",
    "image_width = char_image_width * max_length\n",
    "\n",
    "input_shape = (image_height, image_width)\n",
    "output_shape = (max_length, num_classes)\n",
    "\n",
    "window_width: float = 16\n",
    "window_stride: float = 8\n",
    "    \n",
    "num_windows = ((image_width - window_width) // window_stride) + 1\n",
    "if num_windows < max_length:\n",
    "    raise ValueError(f'Window width/stride need to generate >= {max_length} windows (currently {num_windows})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_window(image, window_width, window_stride):\n",
    "    \"\"\"\n",
    "    Takes (image_height, image_width, 1) input,\n",
    "    Returns (num_windows, image_height, window_width, 1) output, where\n",
    "    num_windows is floor((image_width - window_width) / window_stride) + 1\n",
    "    \"\"\"\n",
    "    patches = tf.image.extract_image_patches(image, \n",
    "                                             sizes=[1, 1, window_width, 1], \n",
    "                                             strides=[1, 1, window_stride, 1], \n",
    "                                             rates=[1, 1, 1, 1], \n",
    "                                             padding='VALID')\n",
    "    patches = tf.transpose(patches, (0, 2, 1, 3))\n",
    "    patches = tf.expand_dims(patches, -1)\n",
    "    \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_width = 12\n",
    "window_stride = 5\n",
    "\n",
    "image_input = layers.Input(shape=input_shape, name='image')\n",
    "image_reshaped = layers.Reshape((image_height, image_width, 1))(image_input)\n",
    "\n",
    "image_patches = layers.Lambda(\n",
    "    slide_window,\n",
    "    arguments={'window_width': window_width, 'window_stride': window_stride}\n",
    ")(image_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lambda/ExpandDims:0' shape=(?, 189, 28, 12, 1) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the actual loss calc occurs here despite it not being an internal Keras loss function\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f5b6fdf8310>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f5b6f570e50>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f5b6f52edd0>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7f5bc0934c90>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x7f5bc0934bd0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f5bc0a32210>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7f5bc09346d0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f5b6f452550>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnet_base = lenet(NetworkInput(input_shape=(image_height, window_width, 1), number_of_classes=num_classes))\n",
    "convnet_base.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image (InputLayer)           [(None, 28, 952)]         0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 952, 1)        0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 189, 28, 12, 1)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 189, 128)          412160    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 189, 256)          198144    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 189, 128)          123648    \n",
      "_________________________________________________________________\n",
      "softmax_output (Dense)       (None, 189, 64)           8256      \n",
      "=================================================================\n",
      "Total params: 742,208\n",
      "Trainable params: 742,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              [(None, 28, 952)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 28, 952, 1)   0           image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 189, 28, 12,  0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 189, 128)     412160      lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 189, 256)     198144      time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 189, 128)     123648      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax_output (Dense)          (None, 189, 64)      8256        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax_output[0][0]             \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 742,208\n",
      "Trainable params: 742,208\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get rid of the last two layers (dropout and softmax)\n",
    "convnet = KerasModel(inputs=convnet_base.inputs, outputs=convnet_base.layers[-2].output)\n",
    "time_distributed_outputs = layers.TimeDistributed(convnet)(image_patches)\n",
    "\n",
    "# Try a single lstm\n",
    "# rnn_outputs = layers.CuDNNLSTM(128, return_sequences=True)(time_distributed_outputs)  # layers.LSTM(128, return_sequences=True)(time_distributed_outputs)\n",
    "\n",
    "# Try one GRU layer\n",
    "# rnn_outputs = layers.CuDNNGRU(256, return_sequences=True)(time_distributed_outputs)\n",
    "\n",
    "# Try two layers of bidirectional GRUs\n",
    "rnn_outputs = layers.Bidirectional(layers.CuDNNGRU(128, return_sequences=True))(time_distributed_outputs)\n",
    "rnn_outputs = layers.Bidirectional(layers.CuDNNGRU(64, return_sequences=True))(rnn_outputs)\n",
    "\n",
    "y_pred = layers.Dense(num_classes, activation='softmax', name='softmax_output')(rnn_outputs)\n",
    "KerasModel(inputs=image_input, outputs=y_pred).summary()\n",
    "\n",
    "# Add ctc specific ipnuts for the training model, the predication model will just need access to `image_input`\n",
    "labels = layers.Input(name='the_labels', shape=[max_length], dtype='float32')\n",
    "input_length = layers.Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = layers.Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "# Keras doesn't currently support loss funcs with extra parameters\n",
    "# so CTC loss is implemented in a lambda layer\n",
    "loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "\n",
    "# clipnorm seems to speeds up convergence\n",
    "sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "\n",
    "model = KerasModel(inputs=[image_input, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "model.compile(optimizer=sgd, loss={'ctc': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Dataset.processed_data_path()/\"emnist_lines\"/\"data.csv\")\n",
    "\n",
    "output_length = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.png</td>\n",
       "      <td>Whats wrong at state _____________</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.png</td>\n",
       "      <td>The transducer itself moves the __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.png</td>\n",
       "      <td>There is an ancient and venerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.png</td>\n",
       "      <td>They destroyed a trading house ___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.png</td>\n",
       "      <td>Alec waited a moment on guard  ___</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image                            sentence\n",
       "0  0.png  Whats wrong at state _____________\n",
       "1  1.png  The transducer itself moves the __\n",
       "2  2.png  There is an ancient and venerable \n",
       "3  3.png  They destroyed a trading house ___\n",
       "4  4.png  Alec waited a moment on guard  ___"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the data and reset their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2336.png</td>\n",
       "      <td>It was all set up so there would _</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8293.png</td>\n",
       "      <td>The latter now furnishes the area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>230.png</td>\n",
       "      <td>He looked at her out of himself __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2934.png</td>\n",
       "      <td>The Latin for example was not ____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3139.png</td>\n",
       "      <td>Where to file  ___________________</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image                            sentence\n",
       "0  2336.png  It was all set up so there would _\n",
       "1  8293.png  The latter now furnishes the area \n",
       "2   230.png  He looked at her out of himself __\n",
       "3  2934.png  The Latin for example was not ____\n",
       "4  3139.png  Where to file  ___________________"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For the sake of debugging let us test only one sentence\n",
    "# df = df.iloc[[0] * len(df)]\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_length = int(len(df) * .2)\n",
    "\n",
    "train_df = df.iloc[valid_length:]\n",
    "valid_df = df.iloc[:valid_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinesDataSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, df, batch_size=32):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) // batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a single batch.\"\"\"\n",
    "        X_data = np.ones([self.batch_size, image_height, image_width])\n",
    "\n",
    "        labels = np.ones([self.batch_size, max_length])\n",
    "        input_length = np.zeros([self.batch_size, 1])\n",
    "        label_length = np.zeros([self.batch_size, 1])\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            index = i + idx\n",
    "            row = self.df.iloc[index]\n",
    "            \n",
    "            image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/row['image'])\n",
    "            image = np.array(image).astype(np.float32).reshape(image_height, image_width)\n",
    "            X_data[i, :, :] = image\n",
    "            \n",
    "            y = [mapping_reversed[char] for char in row['sentence']]\n",
    "            y = to_categorical(y, num_classes).astype(np.int)\n",
    "#             labels[i, :] = y\n",
    "            labels[i, :] = np.argmax(y, axis=-1)\n",
    "#             labels[i, :] = np.asarray(y)\n",
    "            \n",
    "            # input_length refers to your sequence length and label_length refers to the ground truth label length\n",
    "            # TODO :: Not sure what to do with this!\n",
    "            input_length[i] = 189 - 2  # 64  # 34  # 189\n",
    "            \n",
    "            # Find all of the indices in the label that are not blank\n",
    "            empty_at = np.where(y[:, -1] == 1)[0]\n",
    "            # Length of the label is the pos of the first blank, or the max length\n",
    "            if empty_at.shape[0] > 0:\n",
    "                label_length[i] = empty_at[0]\n",
    "            else:\n",
    "                label_length[i] = y.shape[0]\n",
    "            \n",
    "        inputs = {\n",
    "            'image': X_data,\n",
    "            'the_labels': labels,\n",
    "            'input_length': input_length,\n",
    "            'label_length': label_length\n",
    "        }\n",
    "        outputs = {'ctc': np.zeros([self.batch_size])}  # dummy data for dummy loss function\n",
    "\n",
    "        return (inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check progress in TensorBoard by running `tensorboard --logdir=logs/lines_ctc_loss` in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir='../logs/lines_ctc_loss')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "78/78 [==============================] - 58s 739ms/step - loss: 115.4028 - val_loss: 80.0431\n",
      "Epoch 2/10\n",
      "78/78 [==============================] - 45s 579ms/step - loss: 73.8574 - val_loss: 72.9440\n",
      "Epoch 3/10\n",
      "78/78 [==============================] - 45s 582ms/step - loss: 64.6830 - val_loss: 69.7381\n",
      "Epoch 4/10\n",
      "78/78 [==============================] - 45s 580ms/step - loss: 56.3173 - val_loss: 70.5059\n",
      "Epoch 5/10\n",
      "78/78 [==============================] - 45s 578ms/step - loss: 46.6542 - val_loss: 73.0654\n",
      "Epoch 6/10\n",
      "78/78 [==============================] - 45s 577ms/step - loss: 37.9462 - val_loss: 76.8413\n",
      "Epoch 7/10\n",
      "78/78 [==============================] - 45s 574ms/step - loss: 30.2049 - val_loss: 85.2798\n",
      "Epoch 8/10\n",
      "78/78 [==============================] - 45s 576ms/step - loss: 24.3437 - val_loss: 94.1648\n",
      "Epoch 9/10\n",
      "78/78 [==============================] - 45s 574ms/step - loss: 22.6326 - val_loss: 99.1836\n",
      "Epoch 10/10\n",
      "78/78 [==============================] - 45s 574ms/step - loss: 23.1994 - val_loss: 103.0322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'val_loss'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    LinesDataSequence(train_df, batch_size),\n",
    "    steps_per_epoch=len(df) // batch_size,\n",
    "    validation_data=LinesDataSequence(valid_df, batch_size),\n",
    "    validation_steps=len(valid_df) // batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks)\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(model_save_path)\n",
    "\n",
    "# model.trainable = False\n",
    "# model.compile(optimizer=sgd, loss={'ctc': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "# Load weights into the model instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image (InputLayer)           [(None, 28, 952)]         0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 952, 1)        0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 189, 28, 12, 1)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 189, 128)          412160    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 189, 256)          198144    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 189, 128)          123648    \n",
      "_________________________________________________________________\n",
      "softmax_output (Dense)       (None, 189, 64)           8256      \n",
      "=================================================================\n",
      "Total params: 742,208\n",
      "Trainable params: 742,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_pred = KerasModel(inputs=image_input, outputs=y_pred)\n",
    "# model.trainable = False\n",
    "# model.compile(optimizer=sgd, loss={'ctc': lambda y_true, y_pred: y_pred}, metrics=['accuracy'])\n",
    "# model.load_weights(weight_file)\n",
    "\n",
    "model_pred.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_text(labels):\n",
    "    return \"\".join([mapping[c] for c in labels])\n",
    "\n",
    "def decode_predict_ctc(out, top_paths=1):\n",
    "    results = []\n",
    "    beam_width = 5\n",
    "    if beam_width < top_paths:\n",
    "          beam_width = top_paths\n",
    "    for i in range(top_paths):\n",
    "        lables = K.get_value(K.ctc_decode(out, input_length=np.ones(out.shape[0])*out.shape[1],\n",
    "                           greedy=False, beam_width=beam_width, top_paths=top_paths)[0][i])[0]\n",
    "        text = labels_to_text(lables)\n",
    "        results.append(text)\n",
    "        \n",
    "    return results\n",
    "  \n",
    "def predit_on_image(model, img, top_paths=1):\n",
    "    batch = np.expand_dims(img, axis=0)  # Create a fake batch of one image\n",
    "    net_out_value = model.predict(batch)\n",
    "    top_pred_texts = decode_predict_ctc(net_out_value, top_paths)\n",
    "    \n",
    "    return top_pred_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:5438: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['HIe hasn oad af of to forn ahoile ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/df.iloc[index]['image'])\n",
    "image_numpy = np.array(image).astype(np.float32) / 255\n",
    "\n",
    "# sentence = df.iloc[index]['sentence']\n",
    "# y = [mapping_reversed[char] for char in sentence]\n",
    "# y = to_categorical(y, num_classes).astype(np.int)\n",
    "# test_batch_y = y.reshape((1,) + y.shape)\n",
    "\n",
    "results = predit_on_image(model_pred, image_numpy, top_paths=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=952x28 at 0x7F5B6E74B950>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "line-reader",
   "language": "python",
   "name": "line-reader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
