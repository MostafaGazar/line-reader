{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make that we are running the correct version of TensorFlow first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-rc0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 6) # Python ≥3.6 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN model with CTC loss on the generated emnist-lines dataset\n",
    "\n",
    "From Keras examples [image_ocr.py](https://github.com/keras-team/keras/blob/master/examples/image_ocr.py) and [Chengwei's post](https://www.dlology.com/blog/how-to-train-a-keras-model-to-recognize-variable-length-text/) helped me a lot in getting the ctc loss working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "# To be able to write code as if this notebook was one level up in files tree structure.\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.ops import ctc_ops\n",
    "from tensorflow.python.ops import math_ops as tf_math_ops\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model as KerasModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recognizer.datasets import Dataset\n",
    "from recognizer.datasets import EmnistDataset\n",
    "from recognizer.networks import lenet\n",
    "from recognizer.networks import simple\n",
    "from recognizer.networks import NetworkInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 1\n",
    "train_valid_length = 124  # 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = Path(\"../recognizer/weights/lines_ctc_loss.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load characters mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(Dataset.raw_data_path()/\"emnist\"/\"emnist_essentials.json\") as json_file:  \n",
    "    mapping = json.load(json_file)[\"mapping\"]\n",
    "    mapping = {m[0]: m[1] for m in mapping}\n",
    "    mapping[62] = ' '\n",
    "    mapping[63] = '_'\n",
    "    \n",
    "mapping_reversed = {v: k for k, v in mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_text(labels):\n",
    "    return \"\".join([mapping[c] for c in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_image_height = 28\n",
    "char_image_width = 28\n",
    "max_length = 34\n",
    "num_classes = 64\n",
    "image_height = char_image_height\n",
    "image_width = char_image_width * max_length\n",
    "\n",
    "input_shape = (image_height, image_width)\n",
    "output_shape = (max_length, num_classes)\n",
    "\n",
    "window_width: float = 16\n",
    "window_stride: float = 8\n",
    "    \n",
    "num_windows = ((image_width - window_width) // window_stride) + 1\n",
    "if num_windows < max_length:\n",
    "    raise ValueError(f'Window width/stride need to generate >= {max_length} windows (currently {num_windows})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_window(image, window_width, window_stride):\n",
    "    \"\"\"\n",
    "    Takes (image_height, image_width, 1) input,\n",
    "    Returns (num_windows, image_height, window_width, 1) output, where\n",
    "    num_windows is floor((image_width - window_width) / window_stride) + 1\n",
    "    \"\"\"\n",
    "    patches = tf.image.extract_patches(image, \n",
    "                                             sizes=[1, 1, window_width, 1], \n",
    "                                             strides=[1, 1, window_stride, 1], \n",
    "                                             rates=[1, 1, 1, 1], \n",
    "                                             padding='VALID')\n",
    "    patches = tf.transpose(patches, (0, 2, 1, 3))\n",
    "    patches = tf.expand_dims(patches, -1)\n",
    "    \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_width = 12\n",
    "window_stride = 5\n",
    "\n",
    "image_input = layers.Input(shape=input_shape, name='image')\n",
    "image_reshaped = layers.Reshape((image_height, image_width, 1))(image_input)\n",
    "\n",
    "image_patches = layers.Lambda(\n",
    "    slide_window,\n",
    "    arguments={'window_width': window_width, 'window_stride': window_stride}\n",
    ")(image_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lambda/Identity:0' shape=(None, 189, 28, 12, 1) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the actual loss calc occurs here despite it not being an internal Keras loss function\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "def decode_predict_ctc(outs, top_paths=1):\n",
    "    results = []\n",
    "    beam_width = 5\n",
    "    if beam_width < top_paths:\n",
    "          beam_width = top_paths\n",
    "  \n",
    "    for out in outs:\n",
    "        out = np.expand_dims(out, axis=0)\n",
    "        paths = []\n",
    "        for i in range(top_paths):\n",
    "            lables = K.get_value(K.ctc_decode(out, input_length=np.ones(out.shape[0])*out.shape[1],\n",
    "                               greedy=False, beam_width=beam_width, top_paths=top_paths)[0][i])[0]\n",
    "            text = labels_to_text(lables)\n",
    "            paths.append(text)\n",
    "\n",
    "        results.append(paths)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.convolutional.Conv2D at 0x14110c518>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x14110c4a8>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x141162278>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x141162358>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x1411bd3c8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1411bd940>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x1411e3400>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1411e36a0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnet_base = lenet(NetworkInput(input_shape=(image_height, window_width, 1), number_of_classes=num_classes))\n",
    "convnet_base.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image (InputLayer)           [(None, 28, 952)]         0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 952, 1)        0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 189, 28, 12, 1)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 189, 128)          412160    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 189, 128)          131584    \n",
      "_________________________________________________________________\n",
      "softmax_output (Dense)       (None, 189, 64)           8256      \n",
      "=================================================================\n",
      "Total params: 552,000\n",
      "Trainable params: 552,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/mostafagazar/.local/share/virtualenvs/line-reader-pnRtY6V4/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1486: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              [(None, 28, 952)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 28, 952, 1)   0           image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 189, 28, 12,  0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 189, 128)     412160      lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 189, 128)     131584      time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "softmax_output (Dense)          (None, 189, 64)      8256        lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax_output[0][0]             \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 552,000\n",
      "Trainable params: 552,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get rid of the last two layers (dropout and softmax)\n",
    "convnet = KerasModel(inputs=convnet_base.inputs, outputs=convnet_base.layers[-2].output)\n",
    "time_distributed_outputs = layers.TimeDistributed(convnet)(image_patches)\n",
    "\n",
    "# Try a single lstm\n",
    "rnn_outputs = layers.LSTM(128, return_sequences=True)(time_distributed_outputs)\n",
    "\n",
    "# Try one GRU layer\n",
    "# rnn_outputs = layers.GRU(256, return_sequences=True)(time_distributed_outputs)\n",
    "\n",
    "# Try two layers of bidirectional GRUs\n",
    "# rnn_outputs = layers.Bidirectional(layers.GRU(128, return_sequences=True))(time_distributed_outputs)\n",
    "# rnn_outputs = layers.Bidirectional(layers.GRU(64, return_sequences=True))(rnn_outputs)\n",
    "\n",
    "y_pred = layers.Dense(num_classes, activation='softmax', name='softmax_output')(rnn_outputs)\n",
    "KerasModel(inputs=image_input, outputs=y_pred).summary()\n",
    "\n",
    "# Add ctc specific ipnuts for the training model, the predication model will just need access to `image_input`\n",
    "labels = layers.Input(name='the_labels', shape=[max_length], dtype='float32')\n",
    "input_length = layers.Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = layers.Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "# Keras doesn't currently support loss funcs with extra parameters\n",
    "# so CTC loss is implemented in a lambda layer\n",
    "loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "\n",
    "model = KerasModel(inputs=[image_input, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "# # clipnorm seems to speeds up convergence\n",
    "sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "model.compile(optimizer=sgd, loss={'ctc': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Dataset.processed_data_path()/\"emnist_lines\"/\"data.csv\")\n",
    "\n",
    "output_length = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.png</td>\n",
       "      <td>Whats wrong at state _____________</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.png</td>\n",
       "      <td>The transducer itself moves the __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.png</td>\n",
       "      <td>There is an ancient and venerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.png</td>\n",
       "      <td>They destroyed a trading house ___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.png</td>\n",
       "      <td>Alec waited a moment on guard  ___</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image                            sentence\n",
       "0  0.png  Whats wrong at state _____________\n",
       "1  1.png  The transducer itself moves the __\n",
       "2  2.png  There is an ancient and venerable \n",
       "3  3.png  They destroyed a trading house ___\n",
       "4  4.png  Alec waited a moment on guard  ___"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the data and reset their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7563.png</td>\n",
       "      <td>When the Achaeans entertained ____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6270.png</td>\n",
       "      <td>Look at Pete Alcorn who hadnt ____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2919.png</td>\n",
       "      <td>As controls other sections were __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3442.png</td>\n",
       "      <td>She knelt out of reverence for ___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7062.png</td>\n",
       "      <td>So help me God Ill get him  ______</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image                            sentence\n",
       "0  7563.png  When the Achaeans entertained ____\n",
       "1  6270.png  Look at Pete Alcorn who hadnt ____\n",
       "2  2919.png  As controls other sections were __\n",
       "3  3442.png  She knelt out of reverence for ___\n",
       "4  7062.png  So help me God Ill get him  ______"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For the sake of debugging let us test only one sentence\n",
    "# df = df.iloc[[0] * len(df)]\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 24)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[:train_valid_length]\n",
    "\n",
    "valid_length = int(len(df) * .2)\n",
    "\n",
    "train_df = df.iloc[valid_length:]\n",
    "valid_df = df.iloc[:valid_length]\n",
    "\n",
    "len(train_df), valid_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinesDataSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, df, batch_size=32):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) // batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a single batch.\"\"\"\n",
    "        X_data = np.ones([self.batch_size, image_height, image_width])\n",
    "\n",
    "        labels = np.ones([self.batch_size, max_length])\n",
    "        input_length = np.zeros([self.batch_size, 1])\n",
    "        label_length = np.zeros([self.batch_size, 1])\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            index = i + idx\n",
    "            row = self.df.iloc[index]\n",
    "            \n",
    "            image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/row['image'])\n",
    "            image = np.array(image).astype(np.float32).reshape(image_height, image_width)\n",
    "            X_data[i, :, :] = image\n",
    "            \n",
    "            y = [mapping_reversed[char] for char in row['sentence']]\n",
    "            y = to_categorical(y, num_classes).astype(np.int)\n",
    "            labels[i, :] = np.argmax(y, axis=-1)\n",
    "            \n",
    "            # input_length refers to your sequence length and label_length refers to the ground truth label length\n",
    "            # TODO :: Not sure what to do with this!\n",
    "            input_length[i] = 189 - 2  # 64  # 34  # 189\n",
    "            \n",
    "            # Find all of the indices in the label that are not blank\n",
    "            empty_at = np.where(y[:, -1] == 1)[0]\n",
    "            # Length of the label is the pos of the first blank, or the max length\n",
    "            if empty_at.shape[0] > 0:\n",
    "                label_length[i] = empty_at[0]\n",
    "            else:\n",
    "                label_length[i] = y.shape[0]\n",
    "            \n",
    "        inputs = {\n",
    "            'image': X_data,\n",
    "            'the_labels': labels,\n",
    "            'input_length': input_length,\n",
    "            'label_length': label_length\n",
    "        }\n",
    "        outputs = {'ctc': np.zeros([self.batch_size])}  # dummy data for dummy loss function\n",
    "\n",
    "        return (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = LinesDataSequence(train_df, batch_size)\n",
    "steps_per_epoch = len(train_df) // batch_size\n",
    "validation_sequence = LinesDataSequence(valid_df, batch_size)\n",
    "validation_steps = len(valid_df) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(validation_sequence)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (sequence, _) in enumerate(list(validation_sequence)):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "\n",
    "\n",
    "class ValidationDistanceCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        mean_distance = 0.\n",
    "        \n",
    "        model_pred = KerasModel(inputs=image_input, outputs=y_pred)\n",
    "        net_out_values = model_pred.predict_generator(\n",
    "            validation_sequence,\n",
    "            steps=validation_steps\n",
    "        )\n",
    "        text_pred = decode_predict_ctc(net_out_values, top_paths=1)\n",
    "        \n",
    "        index = 0\n",
    "        for sequence, _ in validation_sequence:\n",
    "            for labels in sequence['the_labels']:\n",
    "                text = labels_to_text(labels)\n",
    "                mean_distance += editdistance.eval(text_pred[index], text)\n",
    "                index += 1\n",
    "\n",
    "        mean_distance /= index\n",
    "        \n",
    "        print(f\"\\nEvaluating: loss {logs['loss']:.4f}, mean_distance {mean_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check progress in TensorBoard by running `tensorboard --logdir=logs/lines_ctc_loss` in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir='../logs/lines_ctc_loss'),\n",
    "    ValidationDistanceCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/6 [========================>.....] - ETA: 1s - loss: 377.9865\n",
      "Evaluating: loss 377.0344, mean_distance 34.0\n",
      "6/6 [==============================] - 11s 2s/step - loss: 377.0344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss'])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_sequence,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "#     validation_data=validation_sequence,\n",
    "#     validation_steps=validation_steps,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks)\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(model_save_path)\n",
    "\n",
    "# model.trainable = False\n",
    "# model.compile(optimizer=sgd, loss={'ctc': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "# Load weights into the model instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image (InputLayer)           [(None, 28, 952)]         0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 952, 1)        0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 189, 28, 12, 1)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 189, 128)          412160    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 189, 128)          131584    \n",
      "_________________________________________________________________\n",
      "softmax_output (Dense)       (None, 189, 64)           8256      \n",
      "=================================================================\n",
      "Total params: 552,000\n",
      "Trainable params: 552,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_pred = KerasModel(inputs=image_input, outputs=y_pred)\n",
    "# model.trainable = False\n",
    "# model.compile(optimizer=sgd, loss={'ctc': lambda y_true, y_pred: y_pred}, metrics=['accuracy'])\n",
    "# model.load_weights(weight_file)\n",
    "\n",
    "model_pred.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predit_on_image(model, img, top_paths=1):\n",
    "    batch = np.asarray([image_numpy, image_numpy]) #np.expand_dims(img, axis=0)  # Create a fake batch of one image\n",
    "    net_out_value = model.predict(batch)\n",
    "    top_pred_texts = decode_predict_ctc(net_out_value, top_paths)\n",
    "    \n",
    "    return top_pred_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 189, 64)\n",
      "(1, 189, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('When the Achaeans entertained ____', [['', '1', '0'], ['', '1', '0']])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/df.iloc[index]['image'])\n",
    "image_numpy = np.array(image).astype(np.float32) / 255\n",
    "\n",
    "sentence = df.iloc[index]['sentence']\n",
    "# y = [mapping_reversed[char] for char in sentence]\n",
    "# y = to_categorical(y, num_classes).astype(np.int)\n",
    "# test_batch_y = y.reshape((1,) + y.shape)\n",
    "\n",
    "results = predit_on_image(model_pred, image_numpy, top_paths=3)\n",
    "sentence, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=952x28 at 0x13C252C50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "line-reader",
   "language": "python",
   "name": "line-reader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
