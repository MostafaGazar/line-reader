{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make that we are running the correct version of TensorFlow first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-alpha0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 6) # Python ≥3.6 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train basic model on the generated emnist-lines dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "# To be able to write code as if this notebook was one level up in files tree structure.\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.ops import ctc_ops\n",
    "from tensorflow.python.ops import math_ops as tf_math_ops\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, Reshape, TimeDistributed, LSTM\n",
    "from tensorflow.keras.models import Model as KerasModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recognizer.datasets import Dataset\n",
    "from recognizer.datasets import EmnistDataset\n",
    "from recognizer.networks import lenet\n",
    "from recognizer.networks import simple\n",
    "from recognizer.networks import NetworkInput\n",
    "from recognizer.models import CTCModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 4  # 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = Path(\"../recognizer/weights/lines_time_distributed_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load characters mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(Dataset.raw_data_path()/\"emnist\"/\"emnist_essentials.json\") as json_file:  \n",
    "    mapping = json.load(json_file)[\"mapping\"]\n",
    "    mapping = {m[0]: m[1] for m in mapping}\n",
    "    mapping[62] = ' '\n",
    "    mapping[63] = '_'\n",
    "    \n",
    "mapping_reversed = {v: k for k, v in mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_image_height = 28\n",
    "char_image_width = 28\n",
    "max_length = 34\n",
    "num_classes = 64\n",
    "image_height = char_image_height\n",
    "image_width = char_image_width * max_length\n",
    "\n",
    "input_shape = (image_height, image_width)\n",
    "output_shape = (max_length, num_classes)\n",
    "\n",
    "window_width: float = 12\n",
    "window_stride: float = 5\n",
    "    \n",
    "num_windows = ((image_width - window_width) // window_stride) + 1\n",
    "if num_windows < max_length:\n",
    "    raise ValueError(f'Window width/stride need to generate >= {max_length} windows (currently {num_windows})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_window(image, window_width, window_stride):\n",
    "    \"\"\"\n",
    "    Takes (image_height, image_width, 1) input,\n",
    "    Returns (num_windows, image_height, window_width, 1) output, where\n",
    "    num_windows is floor((image_width - window_width) / window_stride) + 1\n",
    "    \"\"\"\n",
    "    patches = tf.image.extract_image_patches(image, \n",
    "                                             sizes=[1, 1, window_width, 1], \n",
    "                                             strides=[1, 1, window_stride, 1], \n",
    "                                             rates=[1, 1, 1, 1], \n",
    "                                             padding='VALID')\n",
    "    patches = tf.transpose(patches, (0, 2, 1, 3))\n",
    "    patches = tf.expand_dims(patches, -1)\n",
    "    \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_width = 12\n",
    "window_stride = 5\n",
    "\n",
    "image_input = Input(shape=input_shape, name='image')\n",
    "image_reshaped = Reshape((image_height, image_width, 1))(image_input)\n",
    "\n",
    "image_patches = Lambda(\n",
    "    slide_window,\n",
    "    arguments={'window_width': window_width, 'window_stride': window_stride}\n",
    ")(image_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lambda/ExpandDims:0' shape=(None, 189, 28, 12, 1) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f3716879400>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f370f618eb8>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f370f71feb8>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7f370f71fc18>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x7f370f7340f0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f370f734e10>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7f370f6d05f8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f370f6d0e80>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnet = lenet(NetworkInput(input_shape=(image_height, window_width, 1), number_of_classes=num_classes))\n",
    "convnet.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of the last two layers (dropout and softmax)\n",
    "convnet = KerasModel(inputs=convnet.inputs, outputs=convnet.layers[-2].output)\n",
    "time_distributed_outputs = TimeDistributed(convnet)(image_patches)\n",
    "\n",
    "lstm_outputs = LSTM(128, return_sequences=True)(time_distributed_outputs)\n",
    "\n",
    "softmax_output = Dense(num_classes, activation='softmax', name='softmax_output')(lstm_outputs)\n",
    "\n",
    "model = CTCModel([image_input], [softmax_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Dataset.processed_data_path()/\"emnist_lines\"/\"data.csv\")\n",
    "\n",
    "output_length = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.png</td>\n",
       "      <td>Whats wrong at state _____________</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.png</td>\n",
       "      <td>The transducer itself moves the __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.png</td>\n",
       "      <td>There is an ancient and venerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.png</td>\n",
       "      <td>They destroyed a trading house ___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.png</td>\n",
       "      <td>Alec waited a moment on guard  ___</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image                            sentence\n",
       "0  0.png  Whats wrong at state _____________\n",
       "1  1.png  The transducer itself moves the __\n",
       "2  2.png  There is an ancient and venerable \n",
       "3  3.png  They destroyed a trading house ___\n",
       "4  4.png  Alec waited a moment on guard  ___"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=952x28 at 0x7F370C15D668>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/\"0.png\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 952)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_batch_ctc(batch_x, batch_y):\n",
    "    \"\"\"\n",
    "    Because CTC loss needs to be computed inside of the network, we include information about outputs in the inputs.\n",
    "    \"\"\"\n",
    "    batch_size = batch_y.shape[0]\n",
    "    y_true = np.argmax(batch_y, axis=-1)\n",
    "    \n",
    "    label_lengths = []\n",
    "    for ind in range(batch_size):\n",
    "        # Find all of the indices in the label that are blank\n",
    "        empty_at = np.where(batch_y[ind, :, -1] == 1)[0]\n",
    "        # Length of the label is the pos of the first blank, or the max length\n",
    "        if empty_at.shape[0] > 0:\n",
    "            label_lengths.append(empty_at[0])\n",
    "        else:\n",
    "            label_lengths.append(batch_y.shape[1])\n",
    "\n",
    "    batch_inputs = {\n",
    "        'image': batch_x,\n",
    "        'labels': y_true,\n",
    "        'input_length': np.ones((batch_size, 1)),  # dummy, will be set to num_windows in network\n",
    "        'label_length': np.array(label_lengths)\n",
    "    }\n",
    "    \n",
    "    return batch_inputs, batch_y\n",
    "#     batch_outputs = {\n",
    "#         'ctc_loss': np.zeros(batch_size),  # dummy\n",
    "#         'ctc_decoded': y_true\n",
    "#     }\n",
    "    \n",
    "#     return batch_inputs, batch_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinesDataSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, batch_size=32, augment_fn=None, format_fn=None):\n",
    "        self.df = pd.read_csv(Dataset.processed_data_path()/\"emnist_lines\"/\"data.csv\")\n",
    "        # Shuffle data and reset their index.\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.augment_fn = augment_fn\n",
    "        self.format_fn = format_fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(df) // batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a single batch.\"\"\"\n",
    "        begin = idx * self.batch_size\n",
    "        end = (idx + 1) * self.batch_size\n",
    "\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for index in range(begin, end):\n",
    "            row = self.df.iloc[index]\n",
    "            \n",
    "            image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/row['image'])\n",
    "            x = np.array(image).astype(np.float32).reshape(image_height, image_width)\n",
    "            batch_x.append(x)\n",
    "\n",
    "            y = [mapping_reversed[char] for char in row['sentence']]\n",
    "            y = to_categorical(y, num_classes).astype(np.int)\n",
    "            batch_y.append(y)\n",
    "            \n",
    "        batch_x, batch_y = np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "        return format_batch_ctc(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0802 02:50:33.858611 139875773924736 deprecation.py:323] From /opt/anaconda3/envs/line-reader/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:5151: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0802 02:50:33.964878 139875773924736 deprecation.py:323] From /opt/anaconda3/envs/line-reader/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:5130: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0802 02:50:33.980363 139875773924736 deprecation.py:323] From /opt/anaconda3/envs/line-reader/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:5207: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir='../logs/lines_time_distributed')\n",
    "]\n",
    "\n",
    "# model.compile(optimizer=RMSprop(), metrics=['loss', 'accuracy'])\n",
    "model.compile(Adam(lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              [(None, 28, 952)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 28, 952, 1)   0           image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 189, 28, 12,  0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 189, 128)     412160      lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm (UnifiedLSTM)      (None, 189, 128)     131584      time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "softmax_output (Dense)          (None, 189, 64)      8256        unified_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "labels (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "CTCloss (Lambda)                (None, 1)            0           softmax_output[0][0]             \n",
      "                                                                 labels[0][0]                     \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 552,000\n",
      "Trainable params: 552,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model.model_init.summary()\n",
    "model.model_train.summary()\n",
    "# model.model_eval.summary()\n",
    "# model.model_pred.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check progress in TensorBoard by running `tensorboard --logdir=logs/lines_time_distributed` in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Not enough time for target transition sequence (required: 32, available: 1)0You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs\n\t [[{{node CTCloss/CTCLoss}}]] [Op:__inference_keras_scratch_graph_3271]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f9be99057d8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     workers=2)\n\u001b[0m",
      "\u001b[0;32m~/line-reader/recognizer/models/ctc_model.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, initial_epoch)\u001b[0m\n\u001b[1;32m    300\u001b[0m                                              \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                                              \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                                              max_queue_size=max_queue_size, workers=workers, initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# required??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/line-reader/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/opt/anaconda3/envs/line-reader/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/line-reader/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1257\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/line-reader/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3217\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[1;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[0;32m/opt/anaconda3/envs/line-reader/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/line-reader/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/line-reader/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 415\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    416\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/line-reader/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/line-reader/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Not enough time for target transition sequence (required: 32, available: 1)0You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs\n\t [[{{node CTCloss/CTCLoss}}]] [Op:__inference_keras_scratch_graph_3271]"
     ]
    }
   ],
   "source": [
    "model.fit_generator(LinesDataSequence(batch_size),\n",
    "                    steps_per_epoch=len(df) // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_save_path)\n",
    "\n",
    "# keras.models.save_model(\n",
    "#     model,\n",
    "#     model_save_path,\n",
    "#     overwrite=True,\n",
    "#     include_optimizer=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "line-reader",
   "language": "python",
   "name": "line-reader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
