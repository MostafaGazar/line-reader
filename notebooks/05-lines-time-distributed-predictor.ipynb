{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make that we are running the correct version of TensorFlow first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 6) # Python ≥3.6 required\n",
    "# assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train basic model on the generated emnist-lines dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "# To be able to write code as if this notebook was one level up in files tree structure.\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model as KerasModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recognizer.datasets import Dataset\n",
    "from recognizer.datasets import EmnistDataset\n",
    "from recognizer.networks import lenet\n",
    "from recognizer.networks import simple\n",
    "from recognizer.networks import NetworkInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = Path(\"../recognizer/weights/lines_time_distributed_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load characters mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(Dataset.raw_data_path()/\"emnist\"/\"emnist_essentials.json\") as json_file:  \n",
    "    mapping = json.load(json_file)[\"mapping\"]\n",
    "    mapping = {m[0]: m[1] for m in mapping}\n",
    "    mapping[62] = ' '\n",
    "    mapping[63] = '_'\n",
    "    \n",
    "mapping_reversed = {v: k for k, v in mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_image_height = 28\n",
    "char_image_width = 28\n",
    "max_length = 34\n",
    "num_classes = 64\n",
    "image_height = char_image_height\n",
    "image_width = char_image_width * max_length\n",
    "\n",
    "input_shape = (image_height, image_width)\n",
    "output_shape = (max_length, num_classes)\n",
    "\n",
    "window_width: float = 16\n",
    "window_stride: float = 8\n",
    "    \n",
    "num_windows = ((image_width - window_width) // window_stride) + 1\n",
    "if num_windows < max_length:\n",
    "    raise ValueError(f'Window width/stride need to generate >= {max_length} windows (currently {num_windows})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `tensorflow.keras.image.extract_image_patches` not found.\n"
     ]
    }
   ],
   "source": [
    "tensorflow.keras.image.extract_image_patches??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_window(image, window_width, window_stride):\n",
    "    \"\"\"\n",
    "    Takes (image_height, image_width, 1) input,\n",
    "    Returns (num_windows, image_height, window_width, 1) output, where\n",
    "    num_windows is floor((image_width - window_width) / window_stride) + 1\n",
    "    \"\"\"\n",
    "    patches = tf.image.extract_patches(image, \n",
    "                                       sizes=[1, 1, window_width, 1], \n",
    "                                       strides=[1, 1, window_stride, 1], \n",
    "                                       rates=[1, 1, 1, 1], \n",
    "                                       padding='VALID')\n",
    "    patches = tf.transpose(patches, (0, 2, 1, 3))  # Swap width and height\n",
    "    patches = tf.expand_dims(patches, -1)\n",
    "    \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_width = 28\n",
    "# For simplicity sake, let us just make number of windows = number of max characters in a sentence\n",
    "window_stride = 28. # 14\n",
    "\n",
    "image_input = layers.Input(shape=input_shape, name='image')\n",
    "image_reshaped = layers.Reshape((image_height, image_width, 1))(image_input)\n",
    "\n",
    "image_patches = layers.Lambda(\n",
    "    slide_window,\n",
    "    arguments={'window_width': window_width, 'window_stride': window_stride}\n",
    ")(image_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lambda/Identity:0' shape=(?, 34, 28, 28, 1) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa7274d02d0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa7274d0e10>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fa7274a03d0>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7fa7274be190>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x7fa7274c7b90>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa72748a210>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7fa778a65910>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa77a4af110>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnet_base = lenet(NetworkInput(input_shape=(image_height, window_width, 1), number_of_classes=num_classes))\n",
    "convnet_base.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image (InputLayer)           [(None, 28, 952)]         0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 952, 1)        0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 34, 28, 28, 1)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 34, 128)           1198592   \n",
      "_________________________________________________________________\n",
      "softmax_output (Dense)       (None, 34, 64)            8256      \n",
      "=================================================================\n",
      "Total params: 1,206,848\n",
      "Trainable params: 1,206,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get rid of the last two layers (dropout and softmax)\n",
    "convnet = KerasModel(inputs=convnet_base.inputs, outputs=convnet_base.layers[-2].output)\n",
    "\n",
    "time_distributed_outputs = layers.TimeDistributed(convnet)(image_patches)\n",
    "\n",
    "# TODO :: Add conv2D layer to sum the windows.\n",
    "# time_distributed_outputs = layers.Reshape((67, 128, 1))(time_distributed_outputs)\n",
    "# time_distributed_outputs = layers.Conv2D(max_length, kernel_size=(width, 128), strides=(width, 1), activation='relu', name='sum_windows')(time_distributed_outputs)\n",
    "\n",
    "softmax_output = layers.Dense(num_classes, activation='softmax', name='softmax_output')(time_distributed_outputs)\n",
    "\n",
    "model = KerasModel(\n",
    "        inputs=image_input,\n",
    "        outputs=softmax_output\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load emnist lines and pass it to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Dataset.processed_data_path()/\"emnist_lines\"/\"data.csv\")\n",
    "\n",
    "output_length = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.png</td>\n",
       "      <td>Whats wrong at state _____________</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.png</td>\n",
       "      <td>The transducer itself moves the __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.png</td>\n",
       "      <td>There is an ancient and venerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.png</td>\n",
       "      <td>They destroyed a trading house ___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.png</td>\n",
       "      <td>Alec waited a moment on guard  ___</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image                            sentence\n",
       "0  0.png  Whats wrong at state _____________\n",
       "1  1.png  The transducer itself moves the __\n",
       "2  2.png  There is an ancient and venerable \n",
       "3  3.png  They destroyed a trading house ___\n",
       "4  4.png  Alec waited a moment on guard  ___"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=952x28 at 0x7FA7241D6BD0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(Dataset.processed_data_path()/\"emnist_lines\"/\"0.png\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 952)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(34), Dimension(64)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_numpy = np.array(image).astype(np.float32)\n",
    "test_batch = image_numpy.reshape(1, image_height, image_width, 1)\n",
    "\n",
    "preds = model(test_batch)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=952x28 at 0x7FA7241D6BD0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OenvT0OwTTT0TT0neeTO00000000000000"
     ]
    }
   ],
   "source": [
    "for char_pred in preds[0]:\n",
    "    print(mapping[np.argmax(char_pred)], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the data and reset their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4578.png</td>\n",
       "      <td>And you think you have language __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3059.png</td>\n",
       "      <td>he ran wild with the child gangs _</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1330.png</td>\n",
       "      <td>We should not allow the image of _</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7955.png</td>\n",
       "      <td>You saw them always together _____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8044.png</td>\n",
       "      <td>The planning division will take __</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image                            sentence\n",
       "0  4578.png  And you think you have language __\n",
       "1  3059.png  he ran wild with the child gangs _\n",
       "2  1330.png  We should not allow the image of _\n",
       "3  7955.png  You saw them always together _____\n",
       "4  8044.png  The planning division will take __"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_length = int(len(df) * .2)\n",
    "valid_df = df.iloc[:valid_length]\n",
    "train_df = df.iloc[valid_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1995</td>\n",
       "      <td>1468.png</td>\n",
       "      <td>Parker certainly had much more of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1996</td>\n",
       "      <td>8954.png</td>\n",
       "      <td>Since the writer had not noticed _</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1997</td>\n",
       "      <td>4202.png</td>\n",
       "      <td>It was equally clear that as of __</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1998</td>\n",
       "      <td>3545.png</td>\n",
       "      <td>During a round of target practice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1999</td>\n",
       "      <td>5407.png</td>\n",
       "      <td>Nonspecific staining could be ____</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image                            sentence\n",
       "1995  1468.png  Parker certainly had much more of \n",
       "1996  8954.png  Since the writer had not noticed _\n",
       "1997  4202.png  It was equally clear that as of __\n",
       "1998  3545.png  During a round of target practice \n",
       "1999  5407.png  Nonspecific staining could be ____"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3206.png</td>\n",
       "      <td>In this case it is primarily a ___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2001</td>\n",
       "      <td>6766.png</td>\n",
       "      <td>It struck the 9th green on the ___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2002</td>\n",
       "      <td>7615.png</td>\n",
       "      <td>It is generally an inaccurate ____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2003</td>\n",
       "      <td>4426.png</td>\n",
       "      <td>he tells stories of the Thousand _</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004</td>\n",
       "      <td>6771.png</td>\n",
       "      <td>Social control  __________________</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image                            sentence\n",
       "2000  3206.png  In this case it is primarily a ___\n",
       "2001  6766.png  It struck the 9th green on the ___\n",
       "2002  7615.png  It is generally an inaccurate ____\n",
       "2003  4426.png  he tells stories of the Thousand _\n",
       "2004  6771.png  Social control  __________________"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinesDataSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, df, batch_size=32):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) // batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a single batch.\"\"\"\n",
    "        begin = idx * self.batch_size\n",
    "        end = min(len(df) - 1, (idx + 1) * self.batch_size)\n",
    "\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for index in range(begin, end):\n",
    "            row = self.df.iloc[index]\n",
    "            \n",
    "            image = Image.open(Dataset.processed_data_path()/'emnist_lines'/row['image'])\n",
    "            x = np.array(image).astype(np.float32).reshape(image_height, image_width)\n",
    "            batch_x.append(x)\n",
    "\n",
    "            y = [mapping_reversed[char] for char in row['sentence']]\n",
    "            y = to_categorical(y, num_classes).astype(np.int)\n",
    "            batch_y.append(y)\n",
    "\n",
    "        return np.array(batch_x), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image (InputLayer)           [(None, 28, 952)]         0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 952, 1)        0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 34, 28, 28, 1)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 34, 128)           1198592   \n",
      "_________________________________________________________________\n",
      "softmax_output (Dense)       (None, 34, 64)            8256      \n",
      "=================================================================\n",
      "Total params: 1,206,848\n",
      "Trainable params: 1,206,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check progress in TensorBoard by running `tensorboard --logdir=logs/lines_time_distributed` in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ../logs/lines_time_distributed\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir='../logs/lines_time_distributed')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0826 04:15:35.952363 140358255253248 deprecation.py:323] From /home/jupyter/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "  1/500 [..............................] - ETA: 16:01 - loss: 10.8647 - acc: 0.0239"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 04:15:37.819498 140358255253248 callbacks.py:257] Method (on_train_batch_end) is slow compared to the batch update (0.205099). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 20s 41ms/step - loss: 1.8171 - acc: 0.6934 - val_loss: 0.4348 - val_acc: 0.8022\n",
      "Epoch 2/32\n",
      "500/500 [==============================] - 18s 37ms/step - loss: 0.4421 - acc: 0.8002 - val_loss: 0.3977 - val_acc: 0.8116\n",
      "Epoch 3/32\n",
      "500/500 [==============================] - 18s 37ms/step - loss: 0.4037 - acc: 0.8090 - val_loss: 0.3927 - val_acc: 0.7836\n",
      "Epoch 4/32\n",
      "117/500 [======>.......................] - ETA: 11s - loss: 0.3897 - acc: 0.8050"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d75f85b35269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     callbacks=callbacks)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3441\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3442\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3443\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3444\u001b[0m     return nest.pack_sequence_as(\n\u001b[1;32m   3445\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    560\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 561\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    432\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    433\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 434\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    435\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/line-reader-3vKrZmrE/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    LinesDataSequence(train_df, batch_size),\n",
    "    steps_per_epoch=len(train_df) // batch_size,\n",
    "    validation_data=LinesDataSequence(valid_df, batch_size),\n",
    "    validation_steps=len(valid_df) // batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks)\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c53b3b032837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../recognizer/weights/lines_time_distributed_model.h5')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False\n",
    "\n",
    "# Compile the model for trainable changes to take effect.\n",
    "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(34), Dimension(64)])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(test_batch)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=952x28 at 0x7FA7241D6BD0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whats_wrong_at_state______________"
     ]
    }
   ],
   "source": [
    "for char_pred in preds[0]:\n",
    "    print(mapping[np.argmax(char_pred)], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "line-reader",
   "language": "python",
   "name": "line-reader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
